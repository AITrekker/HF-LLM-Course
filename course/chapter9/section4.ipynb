{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sharing demos with others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, evaluation, and Gradio interface\n!uv pip install datasets evaluate transformers[sentencepiece]\n!uv pip install gradio"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced interface with custom title, description, and examples\n# This demonstrates how to create professional-looking interfaces with rich metadata\ntitle = \"Ask Rick a Question\"\ndescription = \"\"\"\nThe bot was trained to answer questions based on Rick and Morty dialogues. Ask Rick anything!\n<img src=\"https://huggingface.co/spaces/course-demos/Rick_and_Morty_QA/resolve/main/rick.png\" width=200px>\n\"\"\"\n\n# Article section provides additional context or links for users\narticle = \"Check out [the original Rick and Morty Bot](https://huggingface.co/spaces/kingabzpro/Rick_and_Morty_Bot) that this demo is based off of.\"\n\n# Create interface with professional presentation\ngr.Interface(\n    fn=predict,  # Function defined in previous cell\n    inputs=\"textbox\",  # Text input for questions\n    outputs=\"text\",    # Text output for answers\n    title=title,                # Main title displayed at top\n    description=description,    # HTML description with formatting and images\n    article=article,           # Additional information at bottom\n    examples=[[\"What are you doing?\"], [\"Where should we time travel to?\"]],  # Sample inputs\n).launch()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Public sharing with share=True parameter\n# This creates a temporary public URL that others can access for 72 hours\n# Very useful for demonstrations, presentations, or sharing with remote collaborators\ngr.Interface(classify_image, \"image\", \"label\").launch(share=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Custom sketch recognition model - demonstrates loading and using a custom trained model\n# This example shows how to integrate your own PyTorch models with Gradio\nfrom pathlib import Path\nimport torch\nimport gradio as gr\nfrom torch import nn\n\n# Load class labels from file - these are the categories the model can recognize\nLABELS = Path(\"class_names.txt\").read_text().splitlines()\n\n# Define the neural network architecture\n# This CNN is designed for sketch/drawing recognition\nmodel = nn.Sequential(\n    # First convolutional block: 1 input channel (grayscale), 32 output channels\n    nn.Conv2d(1, 32, 3, padding=\"same\"),  # 3x3 convolution with same padding\n    nn.ReLU(),                             # ReLU activation\n    nn.MaxPool2d(2),                       # 2x2 max pooling (downsampling)\n    \n    # Second convolutional block: 32->64 channels\n    nn.Conv2d(32, 64, 3, padding=\"same\"),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    \n    # Third convolutional block: 64->128 channels\n    nn.Conv2d(64, 128, 3, padding=\"same\"),\n    nn.ReLU(),\n    nn.MaxPool2d(2),\n    \n    # Flatten for fully connected layers\n    nn.Flatten(),\n    # Fully connected layers for classification\n    nn.Linear(1152, 256),     # 1152 comes from flattened conv output size\n    nn.ReLU(),\n    nn.Linear(256, len(LABELS)),  # Output layer matches number of classes\n)\n\n# Load pre-trained weights from file\n# map_location=\"cpu\" ensures compatibility regardless of training device\nstate_dict = torch.load(\"pytorch_model.bin\", map_location=\"cpu\")\nmodel.load_state_dict(state_dict, strict=False)\nmodel.eval()  # Set to evaluation mode (disables dropout, batch norm training)\n\n\ndef predict(im):\n    # Preprocess the input image for the model\n    # Convert to tensor, add batch and channel dimensions, normalize to [0,1]\n    x = torch.tensor(im, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.0\n    \n    # Run inference without gradient computation (saves memory and computation)\n    with torch.no_grad():\n        out = model(x)\n    \n    # Convert raw logits to probabilities using softmax\n    probabilities = torch.nn.functional.softmax(out[0], dim=0)\n    \n    # Get top 5 predictions with their confidence scores\n    values, indices = torch.topk(probabilities, 5)\n    \n    # Return as dictionary mapping class names to confidence scores\n    return {LABELS[i]: v.item() for i, v in zip(indices, values)}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Real-time sketch recognition interface with advanced features\n# Demonstrates live prediction and professional interface design\ninterface = gr.Interface(\n    predict,\n    inputs=\"sketchpad\",    # Special input type for drawing/sketching\n    outputs=\"label\",       # Label output shows top predictions with confidence scores\n    theme=\"huggingface\",   # Use Hugging Face's branded theme\n    title=\"Sketch Recognition\",\n    description=\"Who wants to play Pictionary? Draw a common object like a shovel or a laptop, and the algorithm will guess in real time!\",\n    article=\"<p style='text-align: center'>Sketch Recognition | Demo Model</p>\",\n    live=True,            # Enable live prediction: updates as user draws\n)\ninterface.launch(share=True)  # Launch with public sharing enabled"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sharing demos with others",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}