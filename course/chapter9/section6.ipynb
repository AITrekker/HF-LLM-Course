{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Interface features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, evaluation, and Gradio interface\n!uv pip install datasets evaluate transformers[sentencepiece]\n!uv pip install gradio"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stateful chatbot interface - demonstrates conversation memory with state management\n# This creates a chatbot that remembers the conversation history\nimport random\nimport gradio as gr\n\n\ndef chat(message, history):\n    # Initialize history if None (first message)\n    history = history or []\n    \n    # Simple rule-based chatbot responses based on message patterns\n    if message.startswith(\"How many\"):\n        response = random.randint(1, 10)  # Random number for \"how many\" questions\n    elif message.startswith(\"How\"):\n        response = random.choice([\"Great\", \"Good\", \"Okay\", \"Bad\"])  # Random quality rating\n    elif message.startswith(\"Where\"):\n        response = random.choice([\"Here\", \"There\", \"Somewhere\"])  # Random location\n    else:\n        response = \"I don't know\"  # Default response\n    \n    # Add the new exchange to conversation history\n    # Each entry is a tuple: (user_message, bot_response)\n    history.append((message, response))\n    \n    # Return updated history twice: once for display, once for state persistence\n    return history, history\n\n\n# Create chatbot interface with state management\niface = gr.Interface(\n    chat,\n    [\"text\", \"state\"],     # Inputs: current message + conversation state\n    [\"chatbot\", \"state\"],  # Outputs: chat display + updated state\n    allow_screenshot=False,  # Disable screenshot functionality\n    allow_flagging=\"never\",  # Disable user flagging of responses\n)\niface.launch()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Image classification with interpretation features\n# This demonstrates Gradio's built-in model interpretation capabilities\nimport requests\nimport tensorflow as tf\nimport gradio as gr\n\n# Load pre-trained MobileNetV2 model for image classification\n# MobileNetV2 is optimized for mobile/edge devices while maintaining good accuracy\ninception_net = tf.keras.applications.MobileNetV2()\n\n# Download human-readable labels for ImageNet classes\n# ImageNet has 1000 object categories, each with a descriptive label\nresponse = requests.get(\"https://git.io/JJkYN\")\nlabels = response.text.split(\"\\n\")\n\n\ndef classify_image(inp):\n    # Preprocess input image for MobileNetV2\n    # Reshape to batch format: (batch_size, height, width, channels)\n    inp = inp.reshape((-1, 224, 224, 3))\n    \n    # Apply MobileNetV2-specific preprocessing (normalization)\n    inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)\n    \n    # Get model predictions for all 1000 ImageNet classes\n    prediction = inception_net.predict(inp).flatten()\n    \n    # Return dictionary mapping class labels to confidence scores\n    # This format is required for Gradio's Label output component\n    return {labels[i]: float(prediction[i]) for i in range(1000)}\n\n\n# Configure input and output components\nimage = gr.Image(shape=(224, 224))  # Fixed size image input matching model requirements\nlabel = gr.Label(num_top_classes=3)  # Show top 3 predictions with confidence scores\n\ntitle = \"Gradio Image Classification + Interpretation Example\"\n\n# Create interface with built-in interpretation\ngr.Interface(\n    fn=classify_image, \n    inputs=image, \n    outputs=label, \n    interpretation=\"default\",  # Enable automatic model interpretation\n    title=title\n).launch()"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Advanced Interface features",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}