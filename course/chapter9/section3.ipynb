{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Interface class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, evaluation, and Gradio interface\n!uv pip install datasets evaluate transformers[sentencepiece]\n!uv pip install gradio"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Audio processing interface - demonstrates how to work with audio inputs\n# This function reverses audio data by flipping the numpy array\nimport numpy as np\nimport gradio as gr\n\n\ndef reverse_audio(audio):\n    # audio is a tuple: (sample_rate, audio_data)\n    sr, data = audio\n    # np.flipud reverses the audio data array, creating a backwards audio effect\n    reversed_audio = (sr, np.flipud(data))\n    return reversed_audio\n\n\n# Create an audio input component\n# source=\"microphone\": allows real-time recording from user's microphone\n# type=\"numpy\": returns audio as numpy array for processing\n# label: descriptive text for the input\nmic = gr.Audio(source=\"microphone\", type=\"numpy\", label=\"Speak here...\")\n\n# Interface that takes audio input and outputs processed audio\ngr.Interface(reverse_audio, mic, \"audio\").launch()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Music tone generator - demonstrates multiple input types and audio synthesis\n# This creates musical tones based on musical note, octave, and duration parameters\nimport numpy as np\nimport gradio as gr\n\n# Musical notes in chromatic scale starting from C\nnotes = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n\n\ndef generate_tone(note, octave, duration):\n    # Audio settings\n    sr = 48000  # Sample rate: 48kHz for high-quality audio\n    \n    # Calculate frequency using equal temperament tuning\n    # A4 (A in 4th octave) = 440 Hz is the reference frequency\n    a4_freq, tones_from_a4 = 440, 12 * (octave - 4) + (note - 9)\n    # Each semitone is 2^(1/12) times the frequency of the previous note\n    frequency = a4_freq * 2 ** (tones_from_a4 / 12)\n    \n    # Generate time array for the duration\n    duration = int(duration)\n    audio = np.linspace(0, duration, duration * sr)\n    \n    # Generate sine wave tone at calculated frequency\n    # 20000 amplitude for audible volume, convert to 16-bit integer format\n    audio = (20000 * np.sin(audio * (2 * np.pi * frequency))).astype(np.int16)\n    return (sr, audio)\n\n\n# Create interface with multiple input types:\ngr.Interface(\n    generate_tone,\n    [\n        # Dropdown for note selection, returns index of selected note\n        gr.Dropdown(notes, type=\"index\"),\n        # Slider for octave selection (4-6 range covers most common octaves)\n        gr.Slider(minimum=4, maximum=6, step=1),\n        # Text input for duration, expects numeric value\n        gr.Textbox(type=\"number\", value=1, label=\"Duration in seconds\"),\n    ],\n    \"audio\",  # Output type is audio\n).launch()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Speech-to-text interface - demonstrates automatic speech recognition\n# This combines audio processing with NLP pipelines for transcription\nfrom transformers import pipeline\nimport gradio as gr\n\n# Load automatic speech recognition model\n# facebook/wav2vec2-base-960h is a popular ASR model trained on 960 hours of speech\nmodel = pipeline(\"automatic-speech-recognition\")\n\n\ndef transcribe_audio(mic=None, file=None):\n    # Handle two possible audio sources: microphone recording or uploaded file\n    # This flexibility allows users to either record live or upload existing audio\n    if mic is not None:\n        audio = mic\n    elif file is not None:\n        audio = file\n    else:\n        return \"You must either provide a mic recording or a file\"\n    \n    # Transcribe audio to text using the ASR pipeline\n    # The pipeline handles audio preprocessing and model inference automatically\n    transcription = model(audio)[\"text\"]\n    return transcription\n\n\n# Create interface with two optional audio inputs\ngr.Interface(\n    fn=transcribe_audio,\n    inputs=[\n        # Microphone input: records audio and saves as temporary file\n        gr.Audio(source=\"microphone\", type=\"filepath\", optional=True),\n        # File upload input: allows users to upload existing audio files\n        gr.Audio(source=\"upload\", type=\"filepath\", optional=True),\n    ],\n    outputs=\"text\",  # Output the transcribed text\n).launch()"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Understanding the Interface class",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}