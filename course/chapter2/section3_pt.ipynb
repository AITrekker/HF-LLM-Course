{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, and evaluation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Creating a model from scratch - Building configuration and model\n# BertConfig contains all the hyperparameters for the model architecture\n# BertModel creates a model with random weights based on the configuration\nfrom transformers import BertConfig, BertModel\n\n# Building the config - uses default BERT-base parameters\nconfig = BertConfig()\n\n# Building the model from the config - starts with random weights\nmodel = BertModel(config)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the configuration object - shows key model architecture parameters\n# Key parameters include:\n# - hidden_size: size of embeddings (768 for BERT-base)\n# - num_hidden_layers: number of transformer layers (12 for BERT-base)\n# - num_attention_heads: number of attention heads per layer (12 for BERT-base)\n# - intermediate_size: size of feed-forward network (3072 for BERT-base)\n# - max_position_embeddings: maximum sequence length (512 for BERT)\nprint(config)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Important: Creating a model from config gives random weights\n# This model has not been trained and will produce meaningless outputs\n# Useful for research, experimentation, or when you plan to train from scratch\nfrom transformers import BertConfig, BertModel\n\nconfig = BertConfig()\nmodel = BertModel(config)\n\n# Model is randomly initialized! Not suitable for production use without training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Loading a pre-trained model - Downloads trained weights from the Hub\n# \"bert-base-cased\" is a BERT model trained on large text corpora\n# This model has learned meaningful representations and is ready for use\n# Case-sensitive version (distinguishes between \"Apple\" and \"apple\")\nfrom transformers import BertModel\n\nmodel = BertModel.from_pretrained(\"bert-base-cased\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Saving a model locally - stores both config and weights\n# Creates a directory with:\n# - config.json: model configuration\n# - pytorch_model.bin: model weights\n# Allows for offline usage and custom model distribution\nmodel.save_pretrained(\"directory_on_my_computer\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example text sequences for processing\n# These are the raw text inputs that need to be tokenized\nsequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pre-tokenized sequences converted to token IDs\n# Each sequence starts with [CLS] token (101) and ends with [SEP] token (102)\n# These numbers correspond to specific tokens in BERT's vocabulary\n# - 101: [CLS] (classification token)\n# - 102: [SEP] (separator token)\n# - 7592: \"Hello\", 4658: \"Cool\", 3835: \"Nice\"\n# - 999, 1012: punctuation marks (!, .)\nencoded_sequences = [\n    [101, 7592, 999, 102],\n    [101, 4658, 1012, 102],\n    [101, 3835, 999, 102],\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert token IDs to PyTorch tensor\n# Models expect tensors as input, not Python lists\n# The tensor shape will be [batch_size, sequence_length] = [3, 4]\nimport torch\n\nmodel_inputs = torch.tensor(encoded_sequences)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pass the tensor through the model to get embeddings\n# Output contains contextualized embeddings for each token\n# The model automatically handles the forward pass and attention computation\noutput = model(model_inputs)\n\n# output.last_hidden_state contains the final layer embeddings\n# Shape: [batch_size, sequence_length, hidden_size] = [3, 4, 768]"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Models (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}