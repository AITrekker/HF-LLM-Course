{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling multiple sequences (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, and evaluation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Common error: Wrong tensor dimensions for model input\n# Models expect batch dimensions even for single sequences\n# This will fail because input_ids is 1D but model expects 2D [batch_size, seq_len]\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\ninput_ids = torch.tensor(ids)  # This creates a 1D tensor - wrong!\n# This line will fail.\nmodel(input_ids)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Correct approach: Use tokenizer with return_tensors=\"pt\"\n# This automatically adds batch dimension and special tokens ([CLS], [SEP])\n# Notice the tensor shape is [1, sequence_length] not just [sequence_length]\ntokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\nprint(tokenized_inputs[\"input_ids\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Manual fix: Add batch dimension to tensor\n# We manually wrap the IDs in a list to create the batch dimension\n# This creates a 2D tensor with shape [batch_size=1, sequence_length]\n# Now the model can process this input correctly\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\n\ninput_ids = torch.tensor([ids])  # Note the extra brackets for batch dimension\nprint(\"Input IDs:\", input_ids)\n\noutput = model(input_ids)\nprint(\"Logits:\", output.logits)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Problem: Sequences of different lengths can't form a tensor\n# Lists can have different lengths, but tensors require uniform dimensions\n# This structure is invalid for tensor creation\nbatched_ids = [\n    [200, 200, 200],   # Length 3\n    [200, 200]         # Length 2 - inconsistent!\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Padding - making all sequences the same length\n# Add special padding tokens to shorter sequences\n# All sequences now have the same length (3) so we can create a tensor\npadding_id = 100\n\nbatched_ids = [\n    [200, 200, 200],           # Original length 3\n    [200, 200, padding_id],    # Padded from length 2 to 3\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Problem: Padding changes model outputs incorrectly\n# The padded sequence produces different results than the original\n# This happens because the model treats padding tokens as real input\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n\nsequence1_ids = [[200, 200, 200]]\nsequence2_ids = [[200, 200]]\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id],\n]\n\nprint(model(torch.tensor(sequence1_ids)).logits)\nprint(model(torch.tensor(sequence2_ids)).logits)\nprint(model(torch.tensor(batched_ids)).logits)  # Note: second row differs from sequence2_ids!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Attention masks tell the model to ignore padding\n# attention_mask: 1 = real token, 0 = padding token\n# Now the padded sequence produces the same output as the original!\nbatched_ids = [\n    [200, 200, 200],\n    [200, 200, tokenizer.pad_token_id],\n]\n\nattention_mask = [\n    [1, 1, 1],    # All tokens are real\n    [1, 1, 0],    # First two are real, last is padding\n]\n\noutputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\nprint(outputs.logits)  # Second row now matches the unpadded sequence!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Handling sequences that are too long: Truncation\n# Models have maximum sequence lengths (e.g., 512 for BERT)\n# Truncate sequences that exceed this limit by keeping only the first max_length tokens\nsequence = sequence[:max_sequence_length]"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Handling multiple sequences (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}