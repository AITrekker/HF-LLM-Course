{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, and evaluation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Basic tokenization - tokenizer handles the complete preprocessing pipeline\n# This automatically adds special tokens ([CLS], [SEP]) and converts to IDs\n# Returns a dictionary with input_ids, attention_mask, and token_type_ids\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\nmodel_inputs = tokenizer(sequence)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Single sequence processing example\n# Each call to the tokenizer processes the input completely\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\nmodel_inputs = tokenizer(sequence)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Batch processing - handle multiple sequences at once\n# When sequences have different lengths, they need to be padded\n# The tokenizer can handle batching automatically\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\nmodel_inputs = tokenizer(sequences)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Padding strategies - control how sequences are padded in batches\n# padding=\"longest\": pad to the longest sequence in the current batch\n# padding=\"max_length\": pad to the model's maximum length (512 for BERT/DistilBERT)\n# max_length parameter: specify custom maximum length\n\n# Will pad the sequences up to the maximum sequence length\nmodel_inputs = tokenizer(sequences, padding=\"longest\")\n\n# Will pad the sequences up to the model max length\n# (512 for BERT or DistilBERT)\nmodel_inputs = tokenizer(sequences, padding=\"max_length\")\n\n# Will pad the sequences up to the specified max length\nmodel_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Truncation strategies - handle sequences that are too long\n# truncation=True: cut sequences longer than the model's max length\n# max_length + truncation: specify custom length limit\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\n# Will truncate the sequences that are longer than the model max length\n# (512 for BERT or DistilBERT)\nmodel_inputs = tokenizer(sequences, truncation=True)\n\n# Will truncate the sequences that are longer than the specified max length\nmodel_inputs = tokenizer(sequences, max_length=8, truncation=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Return tensor types - specify the format for model consumption\n# return_tensors parameter controls the output format:\n# \"pt\": PyTorch tensors (for PyTorch models)\n# \"tf\": TensorFlow tensors (for TensorFlow/Keras models)  \n# \"np\": NumPy arrays (for other frameworks or manual processing)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\n# Returns PyTorch tensors\nmodel_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n\n# Returns TensorFlow tensors\nmodel_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n\n# Returns NumPy arrays\nmodel_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparing tokenizer() vs manual tokenization\n# Notice the difference: tokenizer() adds special tokens automatically\n# Manual tokenization requires you to handle special tokens yourself\nsequence = \"I've been waiting for a HuggingFace course my whole life.\"\n\nmodel_inputs = tokenizer(sequence)\nprint(model_inputs[\"input_ids\"])  # Includes [CLS] (101) and [SEP] (102) tokens\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(ids)  # Raw tokens without special tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Decode back to text to see the difference\n# The tokenizer() output includes special tokens in the decoded text\n# Manual tokenization output doesn't include special tokens\nprint(tokenizer.decode(model_inputs[\"input_ids\"]))  # With [CLS] and [SEP]\nprint(tokenizer.decode(ids))  # Without special tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete pipeline: tokenization + model inference\n# This combines everything we've learned:\n# 1. Load tokenizer and model\n# 2. Process multiple sequences with padding and truncation\n# 3. Convert to tensors for PyTorch\n# 4. Pass through model using ** to unpack the dictionary\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\ncheckpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n\ntokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\noutput = model(**tokens)  # ** unpacks the dictionary (input_ids, attention_mask, etc.)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Putting it all together (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}