{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, and evaluation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple word-level tokenization using Python's split()\n# This naive approach splits on whitespace but has limitations:\n# - Doesn't handle punctuation well\n# - No handling of unknown words\n# - Fixed vocabulary size issues\ntokenized_text = \"Jim Henson was a puppeteer\".split()\nprint(tokenized_text)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Loading a specific tokenizer class (BertTokenizer)\n# This loads the exact tokenizer designed for BERT models\n# Each model architecture typically has its own tokenizer class\nfrom transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preferred approach: AutoTokenizer automatically selects the right tokenizer\n# AutoTokenizer reads the model's config and loads the appropriate tokenizer class\n# This is more robust and works across different model architectures\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Full tokenization pipeline in one call\n# This handles: text → tokens → IDs → model-ready tensors\n# Returns three key components:\n# - input_ids: numerical token representations\n# - token_type_ids: distinguishes different sentences (for tasks like QA)\n# - attention_mask: indicates real tokens vs padding\ntokenizer(\"Using a Transformer network is simple\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save tokenizer for offline use or distribution\n# Creates files: tokenizer.json, tokenizer_config.json, vocab.txt, etc.\n# Allows loading the tokenizer without internet access\ntokenizer.save_pretrained(\"directory_on_my_computer\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step-by-step tokenization process: text → tokens\n# tokenize() converts text into subword tokens\n# Notice how \"Transformer\" becomes \"transform\" + \"##er\" (WordPiece subword)\n# This handles out-of-vocabulary words by breaking them into known subparts\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nsequence = \"Using a Transformer network is simple\"\ntokens = tokenizer.tokenize(sequence)\n\nprint(tokens)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Convert tokens to numerical IDs\n# Each token maps to a unique number in the tokenizer's vocabulary\n# These IDs are what the model actually processes\nids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(ids)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reverse process: Convert IDs back to text\n# decode() takes token IDs and reconstructs the original text\n# Handles subword reconstruction (e.g., \"transform\" + \"##er\" → \"Transformer\")\n# Useful for understanding model outputs or debugging tokenization\ndecoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\nprint(decoded_string)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tokenizers (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}