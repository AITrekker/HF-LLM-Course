{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast tokenizers in the QA pipeline (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for question-answering tasks with fast tokenizers\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple Question Answering with pipeline\n# The pipeline handles tokenization, model inference, and answer extraction automatically\nfrom transformers import pipeline\n\nquestion_answerer = pipeline(\"question-answering\")\ncontext = \"\"\"\nðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\nbetween them. It's straightforward to train your models with one before loading them for inference with the other.\n\"\"\"\nquestion = \"Which deep learning libraries back ðŸ¤— Transformers?\"\nquestion_answerer(question=question, context=context)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with a much longer context to see how the model handles it\n# Notice the pipeline still finds the correct answer despite the longer text\nlong_context = \"\"\"\nðŸ¤— Transformers: State of the Art NLP\n\nðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\nquestion answering, summarization, translation, text generation and more in over 100 languages.\nIts aim is to make cutting-edge NLP easier to use for everyone.\n\nðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\nthen share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\ncan be modified to enable quick research experiments.\n\nWhy should I use transformers?\n\n1. Easy-to-use state-of-the-art models:\n  - High performance on NLU and NLG tasks.\n  - Low barrier to entry for educators and practitioners.\n  - Few user-facing abstractions with just three classes to learn.\n  - A unified API for using all our pretrained models.\n  - Lower compute costs, smaller carbon footprint:\n\n2. Researchers can share trained models instead of always retraining.\n  - Practitioners can reduce compute time and production costs.\n  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n\n3. Choose the right framework for every part of a model's lifetime:\n  - Train state-of-the-art models in 3 lines of code.\n  - Move a single model between TF2.0/PyTorch frameworks at will.\n  - Seamlessly pick the right framework for training, evaluation and production.\n\n4. Easily customize a model or an example to your needs:\n  - We provide examples for each architecture to reproduce the results published by its original authors.\n  - Model internals are exposed as consistently as possible.\n  - Model files can be used independently of the library for quick experiments.\n\nðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\nbetween them. It's straightforward to train your models with one before loading them for inference with the other.\n\"\"\"\nquestion_answerer(question=question, context=long_context)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Manual QA implementation: Load model and tokenizer for more control\n# This shows what happens under the hood in the QA pipeline\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nmodel_checkpoint = \"distilbert-base-cased-distilled-squad\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n\ninputs = tokenizer(question, context, return_tensors=\"pt\")\noutputs = model(**inputs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# QA models output two sets of logits:\n# start_logits: probability of each token being the START of the answer\n# end_logits: probability of each token being the END of the answer\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\nprint(start_logits.shape, end_logits.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mask question tokens: answers can only come from the context, not the question\n# sequence_ids() shows which tokens belong to question (0) vs context (1)\nimport torch\n\nsequence_ids = inputs.sequence_ids()\n# Mask everything apart from the tokens of the context\nmask = [i != 1 for i in sequence_ids]\n# Unmask the [CLS] token (allow it as a potential \"no answer\" indicator)\nmask[0] = False\nmask = torch.tensor(mask)[None]\n\n# Set masked positions to very low values so they won't be selected\nstart_logits[mask] = -10000\nend_logits[mask] = -10000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert logits to probabilities using softmax\n# This gives us probability distributions over start and end positions\nstart_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\nend_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate all possible answer span scores by multiplying start and end probabilities\n# Each cell [i,j] represents the score for an answer spanning from token i to token j\nscores = start_probabilities[:, None] * end_probabilities[None, :]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Keep only upper triangular part: ensures start_index <= end_index\n# This prevents invalid spans where the \"end\" comes before the \"start\"\nscores = torch.triu(scores)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find the best answer span: highest scoring start-end combination\n# Convert 2D position back to start and end indices\nmax_index = scores.argmax().item()\nstart_index = max_index // scores.shape[1]\nend_index = max_index % scores.shape[1]\nprint(scores[start_index, end_index])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract the actual answer text using offset mapping\n# Fast tokenizers can map token positions back to character positions in original text\ninputs_with_offsets = tokenizer(question, context, return_offsets_mapping=True)\noffsets = inputs_with_offsets[\"offset_mapping\"]\n\nstart_char, _ = offsets[start_index]\n_, end_char = offsets[end_index]\nanswer = context[start_char:end_char]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create the final result in the same format as the pipeline\n# Includes answer text, character positions, and confidence score\nresult = {\n    \"answer\": answer,\n    \"start\": start_char,\n    \"end\": end_char,\n    \"score\": scores[start_index, end_index],\n}\nprint(result)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Problem: Long contexts exceed model's maximum sequence length\n# Check how many tokens the long context produces (461 tokens)\ninputs = tokenizer(question, long_context)\nprint(len(inputs[\"input_ids\"]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Truncate the context to fit model's max length (384 tokens)\n# \"only_second\" means only truncate the context, not the question\n# Notice how the answer is cut off because truncation removed the relevant part\ninputs = tokenizer(question, long_context, max_length=384, truncation=\"only_second\")\nprint(tokenizer.decode(inputs[\"input_ids\"]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstration of sliding window approach with overlapping chunks\n# stride=2: each new chunk overlaps by 2 tokens with the previous chunk\n# This prevents answers from being split across chunk boundaries\nsentence = \"This sentence is not too long but we are going to split it anyway.\"\ninputs = tokenizer(\n    sentence, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n)\n\nfor ids in inputs[\"input_ids\"]:\n    print(tokenizer.decode(ids))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check what additional information is returned with overflowing tokens\n# overflow_to_sample_mapping tracks which original sample each chunk belongs to\nprint(inputs.keys())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# All chunks (7 total) come from the same sample (index 0)\n# This mapping becomes important when processing multiple samples at once\nprint(inputs[\"overflow_to_sample_mapping\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example with multiple sentences: shows how mapping works with multiple samples\n# First 7 chunks from sample 0, next 4 chunks from sample 1\nsentences = [\n    \"This sentence is not too long but we are going to split it anyway.\",\n    \"This sentence is shorter but will still get split.\",\n]\ninputs = tokenizer(\n    sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride=2\n)\n\nprint(inputs[\"overflow_to_sample_mapping\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply sliding window to the long context for question answering\n# stride=128: significant overlap between chunks to ensure no answers are lost\n# return_offsets_mapping=True: needed to extract answer text from original context\ninputs = tokenizer(\n    question,\n    long_context,\n    stride=128,\n    max_length=384,\n    padding=\"longest\",\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for model inference\n# Remove metadata that's not needed for the model and convert to PyTorch tensors\n_ = inputs.pop(\"overflow_to_sample_mapping\")\noffsets = inputs.pop(\"offset_mapping\")\n\ninputs = inputs.convert_to_tensors(\"pt\")\nprint(inputs[\"input_ids\"].shape)  # 2 chunks of 384 tokens each"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run model inference on both chunks simultaneously\n# Each chunk gets its own set of start/end logits\noutputs = model(**inputs)\n\nstart_logits = outputs.start_logits\nend_logits = outputs.end_logits\nprint(start_logits.shape, end_logits.shape)  # [2, 384] for 2 chunks"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced masking for multiple chunks\n# Mask question tokens AND padding tokens from both chunks\nsequence_ids = inputs.sequence_ids()\n# Mask everything apart from the tokens of the context\nmask = [i != 1 for i in sequence_ids]\n# Unmask the [CLS] token\nmask[0] = False\n# Also mask all [PAD] tokens (attention_mask == 0)\nmask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n\nstart_logits[mask] = -10000\nend_logits[mask] = -10000"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert logits to probabilities for both chunks\n# Now we have probability distributions for each chunk separately\nstart_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\nend_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find the best answer candidate in each chunk\n# Compare scores across chunks to find the overall best answer\ncandidates = []\nfor start_probs, end_probs in zip(start_probabilities, end_probabilities):\n    scores = start_probs[:, None] * end_probs[None, :]\n    idx = torch.triu(scores).argmax().item()\n\n    start_idx = idx // scores.shape[1]\n    end_idx = idx % scores.shape[1]\n    score = scores[start_idx, end_idx].item()\n    candidates.append((start_idx, end_idx, score))\n\nprint(candidates)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract final answers from both chunks using offset mapping\n# The second chunk (score 0.97149) contains the correct answer!\n# The first chunk has a low score (0.33867) and extracts irrelevant text\nfor candidate, offset in zip(candidates, offsets):\n    start_token, end_token, score = candidate\n    start_char, _ = offset[start_token]\n    _, end_char = offset[end_token]\n    answer = long_context[start_char:end_char]\n    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n    print(result)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fast tokenizers in the QA pipeline (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}