{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordPiece tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for building a WordPiece tokenizer from scratch\n# - datasets: For loading and processing text datasets\n# - evaluate: For model evaluation metrics\n# - transformers[sentencepiece]: Core library with SentencePiece support\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define the same training corpus to compare WordPiece with BPE\n# WordPiece uses a different algorithm to decide which pairs to merge\n# Instead of pure frequency, it uses a likelihood-based scoring function\ncorpus = [\n    \"This is the Hugging Face Course.\",\n    \"This chapter is about tokenization.\",\n    \"This section shows several tokenizer algorithms.\",\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load BERT tokenizer to understand WordPiece pre-tokenization\n# BERT uses cased tokenization (preserves case) unlike the uncased version\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Pre-tokenize corpus and count word frequencies for WordPiece\n# WordPiece uses the same pre-tokenization step as BERT\n# Note: BERT doesn't use space tokens like GPT-2, it splits on whitespace and punctuation\nfrom collections import defaultdict\n\nword_freqs = defaultdict(int)\nfor text in corpus:\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    new_words = [word for word, offset in words_with_offsets]\n    for word in new_words:\n        word_freqs[word] += 1\n\nword_freqs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Build WordPiece alphabet with special prefix notation\n# WordPiece uses \"##\" to denote word-internal characters (not word beginnings)\n# The first character of each word has no prefix, subsequent characters get \"##\"\nalphabet = []\nfor word in word_freqs.keys():\n    # First character of each word (no prefix)\n    if word[0] not in alphabet:\n        alphabet.append(word[0])\n    # Remaining characters get \"##\" prefix to indicate word-internal position\n    for letter in word[1:]:\n        if f\"##{letter}\" not in alphabet:\n            alphabet.append(f\"##{letter}\")\n\nalphabet.sort()\nalphabet\n\nprint(alphabet)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Initialize vocabulary with BERT's special tokens\n# BERT uses specific special tokens for various purposes:\n# [PAD]: Padding token for batch processing\n# [UNK]: Unknown token for out-of-vocabulary words\n# [CLS]: Classification token (start of sequence)\n# [SEP]: Separator token (end of sequence/between sentences)\n# [MASK]: Mask token for masked language modeling\nvocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Split words according to WordPiece convention\n# First character keeps original form, subsequent characters get \"##\" prefix\n# This preserves information about word boundaries in the tokenization\nsplits = {\n    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n    for word in word_freqs.keys()\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: WordPiece scoring function (different from BPE's pure frequency)\n# WordPiece uses a likelihood-based score: P(pair) / (P(first) * P(second))\n# This prioritizes pairs that are more predictive than random co-occurrence\ndef compute_pair_scores(splits):\n    letter_freqs = defaultdict(int)\n    pair_freqs = defaultdict(int)\n    \n    # Count frequencies of individual letters and pairs\n    for word, freq in word_freqs.items():\n        split = splits[word]\n        if len(split) == 1:\n            letter_freqs[split[0]] += freq\n            continue\n        # Count each character and adjacent pair\n        for i in range(len(split) - 1):\n            pair = (split[i], split[i + 1])\n            letter_freqs[split[i]] += freq\n            pair_freqs[pair] += freq\n        letter_freqs[split[-1]] += freq  # Don't forget the last character\n\n    # Compute WordPiece score: P(pair) / (P(first) * P(second))\n    # Higher scores indicate pairs that occur together more than expected by chance\n    scores = {\n        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n        for pair, freq in pair_freqs.items()\n    }\n    return scores"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Compute initial pair scores to see WordPiece's scoring behavior\n# Notice how scores differ from simple frequency counts\npair_scores = compute_pair_scores(splits)\nfor i, key in enumerate(pair_scores.keys()):\n    print(f\"{key}: {pair_scores[key]}\")\n    if i >= 5:\n        break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Find the highest-scoring pair for first merge\n# WordPiece merges the pair with the highest likelihood score, not necessarily the most frequent\nbest_pair = \"\"\nmax_score = None\nfor pair, score in pair_scores.items():\n    if max_score is None or max_score < score:\n        best_pair = pair\n        max_score = score\n\nprint(best_pair, max_score)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 8: Add the first merged token to vocabulary\n# \"ab\" is created from merging \"a\" + \"##b\"\nvocab.append(\"ab\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 9: WordPiece merge function handles \"##\" prefix properly\n# When merging tokens with \"##\" prefix, the prefix is removed in the result\n# This maintains the WordPiece convention where \"##\" only marks word-internal positions\ndef merge_pair(a, b, splits):\n    for word in word_freqs:\n        split = splits[word]\n        if len(split) == 1:\n            continue\n        i = 0\n        while i < len(split) - 1:\n            if split[i] == a and split[i + 1] == b:\n                # Remove \"##\" prefix from second token when merging\n                merge = a + b[2:] if b.startswith(\"##\") else a + b\n                split = split[:i] + [merge] + split[i + 2 :]\n            else:\n                i += 1\n        splits[word] = split\n    return splits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 10: Apply the first merge and examine the result\n# See how \"about\" is now split as ['ab', '##o', '##u', '##t'] instead of individual characters\nsplits = merge_pair(\"a\", \"##b\", splits)\nsplits[\"about\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 11: Continue WordPiece training until target vocabulary size\n# Main training loop using WordPiece's likelihood-based scoring\nvocab_size = 70\nwhile len(vocab) < vocab_size:\n    scores = compute_pair_scores(splits)\n    best_pair, max_score = \"\", None\n    # Find pair with highest likelihood score\n    for pair, score in scores.items():\n        if max_score is None or max_score < score:\n            best_pair = pair\n            max_score = score\n    \n    # Apply merge and add new token to vocabulary\n    splits = merge_pair(*best_pair, splits)\n    new_token = (\n        best_pair[0] + best_pair[1][2:]  # Remove \"##\" prefix if present\n        if best_pair[1].startswith(\"##\")\n        else best_pair[0] + best_pair[1]\n    )\n    vocab.append(new_token)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 12: Examine the final WordPiece vocabulary\n# Notice how WordPiece discovered different subwords compared to BPE\n# The likelihood-based scoring leads to different merge decisions\nprint(vocab)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 13: WordPiece encoding function using greedy longest-match\n# WordPiece tokenization uses a greedy approach: find the longest token that matches\n# If no token matches, use [UNK] for unknown words\ndef encode_word(word):\n    tokens = []\n    while len(word) > 0:\n        i = len(word)\n        # Try progressively shorter substrings until we find a match in vocabulary\n        while i > 0 and word[:i] not in vocab:\n            i -= 1\n        if i == 0:\n            # No substring found in vocabulary - this is an unknown word\n            return [\"[UNK]\"]\n        tokens.append(word[:i])\n        word = word[i:]  # Continue with remaining part of word\n        if len(word) > 0:\n            word = f\"##{word}\"  # Add \"##\" prefix for word-internal tokens\n    return tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 14: Test the WordPiece encoding function\n# \"Hugging\" was in our training data and gets nicely segmented\n# \"HOgging\" was not seen, so it becomes [UNK] (unknown token)\nprint(encode_word(\"Hugging\"))\nprint(encode_word(\"HOgging\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 15: Complete tokenization function for full text\n# Combines pre-tokenization with WordPiece encoding\ndef tokenize(text):\n    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n    return sum(encoded_words, [])  # Flatten list of lists"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 16: Test our WordPiece tokenizer on new text\n# Notice the \"##\" prefixes indicating word-internal tokens\n# The exclamation mark \"!\" becomes [UNK] since it wasn't in our training vocabulary\ntokenize(\"This is the Hugging Face course!\")"
  }
 ],
 "metadata": {
  "colab": {
   "name": "WordPiece tokenization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}