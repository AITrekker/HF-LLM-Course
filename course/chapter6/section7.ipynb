{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for building a Unigram tokenizer from scratch\n# - datasets: For loading and processing text datasets\n# - evaluate: For model evaluation metrics\n# - transformers[sentencepiece]: Core library with SentencePiece support (Unigram uses SentencePiece)\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the same training corpus to compare Unigram with BPE and WordPiece\n# Unigram starts with a large vocabulary and progressively removes the least useful tokens\n# This is opposite to BPE/WordPiece which start small and grow the vocabulary\ncorpus = [\n    \"This is the Hugging Face Course.\",\n    \"This chapter is about tokenization.\",\n    \"This section shows several tokenizer algorithms.\",\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load XLNet tokenizer to understand Unigram pre-tokenization\n# XLNet uses the Unigram algorithm with SentencePiece implementation\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Pre-tokenize corpus and count word frequencies for Unigram\n# XLNet/SentencePiece uses a different pre-tokenization approach than BERT or GPT-2\nfrom collections import defaultdict\n\nword_freqs = defaultdict(int)\nfor text in corpus:\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    new_words = [word for word, offset in words_with_offsets]\n    for word in new_words:\n        word_freqs[word] += 1\n\nword_freqs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Generate initial large vocabulary for Unigram training\n# Unlike BPE/WordPiece, Unigram starts with ALL possible substrings\n# Then it removes the least useful ones to reach the target vocabulary size\nchar_freqs = defaultdict(int)\nsubwords_freqs = defaultdict(int)\nfor word, freq in word_freqs.items():\n    for i in range(len(word)):\n        char_freqs[word[i]] += freq\n        # Generate all subwords of length 2 or more\n        for j in range(i + 2, len(word) + 1):\n            subwords_freqs[word[i:j]] += freq\n\n# Sort subwords by frequency to prioritize common patterns\nsorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\nsorted_subwords[:10]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Create initial token frequencies combining characters and top subwords\n# Limit to top 300 subwords plus all individual characters to prevent explosion\n# This forms our starting vocabulary that will be pruned down\ntoken_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\ntoken_freqs = {token: freq for token, freq in token_freqs}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Convert frequencies to Unigram model probabilities\n# Unigram uses negative log-likelihood as the scoring function\n# Higher frequency tokens get lower (better) scores\nfrom math import log\n\ntotal_sum = sum([freq for token, freq in token_freqs.items()])\nmodel = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Unigram encoding using dynamic programming (Viterbi algorithm)\n# Unlike BPE/WordPiece greedy approaches, Unigram finds optimal segmentation\n# Uses dynamic programming to find the segmentation with minimum total score\ndef encode_word(word, model):\n    # best_segmentations[i] stores the best way to segment word[:i]\n    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n        {\"start\": None, \"score\": None} for _ in range(len(word))\n    ]\n    \n    for start_idx in range(len(word)):\n        best_score_at_start = best_segmentations[start_idx][\"score\"]\n        if best_score_at_start is None:\n            continue\n            \n        # Try all possible tokens starting from start_idx\n        for end_idx in range(start_idx + 1, len(word) + 1):\n            token = word[start_idx:end_idx]\n            if token in model:\n                score = model[token] + best_score_at_start\n                # Update if this gives a better segmentation ending at end_idx\n                if (\n                    best_segmentations[end_idx][\"score\"] is None\n                    or best_segmentations[end_idx][\"score\"] > score\n                ):\n                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n\n    # Reconstruct the optimal segmentation by backtracking\n    segmentation = best_segmentations[-1]\n    if segmentation[\"score\"] is None:\n        return [\"<unk>\"], None  # No valid tokenization found\n\n    score = segmentation[\"score\"]\n    start = segmentation[\"start\"]\n    end = len(word)\n    tokens = []\n    while start != 0:\n        tokens.insert(0, word[start:end])\n        next_start = best_segmentations[start][\"start\"]\n        end = start\n        start = next_start\n    tokens.insert(0, word[start:end])\n    return tokens, score"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Test the Unigram encoding on example words\n# Notice the different segmentation strategies compared to BPE/WordPiece\n# \"Hopefully\" falls back to character-level due to limited vocabulary\n# \"This\" is found as a complete token with low score (high frequency)\nprint(encode_word(\"Hopefully\", model))\nprint(encode_word(\"This\", model))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Compute total loss function for current model\n# Unigram measures model quality by total negative log-likelihood over corpus\n# Lower loss means better model fit to the training data\ndef compute_loss(model):\n    loss = 0\n    for word, freq in word_freqs.items():\n        _, word_loss = encode_word(word, model)\n        loss += freq * word_loss  # Weight by word frequency\n    return loss"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 8: Compute initial loss with full vocabulary\n# This establishes baseline performance before vocabulary pruning\ncompute_loss(model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 9: Function to evaluate impact of removing each token\n# Unigram removes tokens that contribute least to model performance\n# We compute how much loss increases when each token is removed\nimport copy\n\ndef compute_scores(model):\n    scores = {}\n    model_loss = compute_loss(model)\n    for token, score in model.items():\n        # Always keep single character tokens (can't be removed)\n        if len(token) == 1:\n            continue\n        # Create model without this token and measure loss increase\n        model_without_token = copy.deepcopy(model)\n        _ = model_without_token.pop(token)\n        scores[token] = compute_loss(model_without_token) - model_loss\n    return scores"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 10: Examine token removal scores\n# Tokens with score 0 don't help the model - they can be safely removed\n# Higher scores indicate more important tokens for the model\nscores = compute_scores(model)\nprint(scores[\"ll\"])   # \"ll\" contributes positively to model fit\nprint(scores[\"his\"])  # \"his\" contributes nothing (score = 0)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 11: Main Unigram training loop - iteratively remove least useful tokens\n# This is the core of Unigram training: start big, shrink to target size\n# Remove tokens with lowest scores (least impact on model performance)\npercent_to_remove = 0.1  # Remove 10% of tokens each iteration\nwhile len(model) > 100:\n    scores = compute_scores(model)\n    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n    # Remove the least useful tokens (lowest scores first)\n    for i in range(int(len(model) * percent_to_remove)):\n        _ = token_freqs.pop(sorted_scores[i][0])\n\n    # Recompute model probabilities with reduced vocabulary\n    total_sum = sum([freq for token, freq in token_freqs.items()])\n    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 12: Complete Unigram tokenization function\n# Combines pre-tokenization with optimal Unigram segmentation\ndef tokenize(text, model):\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    pre_tokenized_text = [word for word, offset in words_with_offsets]\n    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n    return sum(encoded_words, [])  # Flatten list of lists\n\n# Test the final Unigram tokenizer\ntokenize(\"This is the Hugging Face course.\", model)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Unigram tokenization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}