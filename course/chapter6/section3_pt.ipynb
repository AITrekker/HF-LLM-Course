{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast tokenizers' special powers (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for working with fast tokenizers and NER models\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load a BERT tokenizer and process example text\n# BatchEncoding contains tokens + additional metadata for fast tokenizers\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nexample = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\nencoding = tokenizer(example)\nprint(type(encoding))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if the tokenizer is a \"fast\" tokenizer (implemented in Rust for speed)\n# Fast tokenizers provide additional features like offset mapping and word alignment\ntokenizer.is_fast"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if the encoding was created by a fast tokenizer\n# Only fast tokenizers can provide offset mapping and alignment features\nencoding.is_fast"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get the actual tokens created by the tokenizer\n# Notice how \"Sylvain\" is split into subword tokens: \"S\", \"##yl\", \"##va\", \"##in\"\nencoding.tokens()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get word IDs: maps each token back to its original word in the input\n# None = special tokens ([CLS], [SEP]), numbers = word index (0-based)\n# Multiple tokens can map to the same word (e.g., \"Sylvain\" -> word 3)\nencoding.word_ids()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find character positions of word 3 (\"Sylvain\") in the original text\n# This allows mapping from word index back to the original string\nstart, end = encoding.word_to_chars(3)\nexample[start:end]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Token Classification pipeline for Named Entity Recognition (NER)\n# Without aggregation, each subword token gets its own prediction\n# Notice how \"Sylvain\" is split into multiple tokens with separate predictions\nfrom transformers import pipeline\n\ntoken_classifier = pipeline(\"token-classification\")\ntoken_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Same pipeline but with aggregation strategy to group subword tokens\n# \"simple\" strategy merges tokens belonging to the same entity\n# Now \"Sylvain\" and \"Hugging Face\" appear as single entities with combined scores\nfrom transformers import pipeline\n\ntoken_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\ntoken_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Manual approach: Load model and tokenizer directly for more control\n# This gives us access to raw model outputs and probabilities\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification\n\nmodel_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n\nexample = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\ninputs = tokenizer(example, return_tensors=\"pt\")\noutputs = model(**inputs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine tensor shapes: \n# inputs: [batch_size=1, sequence_length=19] - 19 tokens including special tokens\n# outputs: [batch_size=1, sequence_length=19, num_labels=9] - 9 possible entity labels\nprint(inputs[\"input_ids\"].shape)\nprint(outputs.logits.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert model outputs to probabilities and predictions\n# Softmax converts logits to probabilities, argmax gets the highest-scoring class\nimport torch\n\nprobabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\npredictions = outputs.logits.argmax(dim=-1)[0].tolist()\nprint(predictions)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mapping from prediction IDs to human-readable labels\n# 0='O' (Outside), B-/I- prefix indicates Beginning/Inside of entity\n# Entity types: PER (Person), ORG (Organization), LOC (Location), MISC (Miscellaneous)\nmodel.config.id2label"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract entity predictions with their confidence scores\n# Only include non-\"O\" (non-Outside) predictions that represent actual entities\nresults = []\ntokens = inputs.tokens()\n\nfor idx, pred in enumerate(predictions):\n    label = model.config.id2label[pred]\n    if label != \"O\":\n        results.append(\n            {\"entity\": label, \"score\": probabilities[idx][pred], \"word\": tokens[idx]}\n        )\n\nprint(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get offset mapping: character positions for each token in the original text\n# This allows us to map tokens back to their exact positions in the input string\ninputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\ninputs_with_offsets[\"offset_mapping\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify offset mapping: extract characters at positions 12-14 from original text\n# This should correspond to the \"##yl\" token from \"Sylvain\"\nexample[12:14]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enhanced results with character positions for each entity token\n# Now we can precisely locate where each entity appears in the original text\nresults = []\ninputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\ntokens = inputs_with_offsets.tokens()\noffsets = inputs_with_offsets[\"offset_mapping\"]\n\nfor idx, pred in enumerate(predictions):\n    label = model.config.id2label[pred]\n    if label != \"O\":\n        start, end = offsets[idx]\n        results.append(\n            {\n                \"entity\": label,\n                \"score\": probabilities[idx][pred],\n                \"word\": tokens[idx],\n                \"start\": start,\n                \"end\": end,\n            }\n        )\n\nprint(results)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify that positions 33-45 extract \"Hugging Face\" from the original text\n# This demonstrates how offset mapping allows precise entity extraction\nexample[33:45]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Advanced entity grouping: Group subword tokens into complete entities\n# This manually implements what the \"simple\" aggregation strategy does automatically\nimport numpy as np\n\nresults = []\ninputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\ntokens = inputs_with_offsets.tokens()\noffsets = inputs_with_offsets[\"offset_mapping\"]\n\nidx = 0\nwhile idx < len(predictions):\n    pred = predictions[idx]\n    label = model.config.id2label[pred]\n    if label != \"O\":\n        # Remove the B- or I- prefix to get the entity type\n        label = label[2:]\n        start, _ = offsets[idx]\n\n        # Collect all consecutive tokens with the same entity label (I-label)\n        all_scores = []\n        while (\n            idx < len(predictions)\n            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n        ):\n            all_scores.append(probabilities[idx][pred])\n            _, end = offsets[idx]\n            idx += 1\n\n        # Average the confidence scores of all tokens in the entity\n        score = np.mean(all_scores).item()\n        # Extract the complete entity text from the original string\n        word = example[start:end]\n        results.append(\n            {\n                \"entity_group\": label,\n                \"score\": score,\n                \"word\": word,\n                \"start\": start,\n                \"end\": end,\n            }\n        )\n    idx += 1\n\nprint(results)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fast tokenizers' special powers (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}