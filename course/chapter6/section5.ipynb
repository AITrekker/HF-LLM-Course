{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte-Pair Encoding tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for building a Byte-Pair Encoding (BPE) tokenizer from scratch\n# - datasets: For loading and processing text datasets\n# - evaluate: For model evaluation metrics  \n# - transformers[sentencepiece]: Core library with SentencePiece support\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a small training corpus to demonstrate BPE algorithm step-by-step\n# This corpus will be used to learn which character pairs to merge most frequently\n# BPE starts with individual characters and iteratively merges the most common pairs\ncorpus = [\n    \"This is the Hugging Face Course.\",\n    \"This chapter is about tokenization.\",\n    \"This section shows several tokenizer algorithms.\",\n    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load GPT-2 tokenizer to understand its pre-tokenization step\n# GPT-2 uses BPE and we'll replicate its approach\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Pre-tokenize the corpus and count word frequencies\n# Pre-tokenization splits text into words, preserving spaces as 'Ġ' tokens\n# This frequency counting helps BPE prioritize merges for common word patterns\nfrom collections import defaultdict\n\nword_freqs = defaultdict(int)\n\nfor text in corpus:\n    # Pre-tokenize each sentence using GPT-2's method\n    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    new_words = [word for word, offset in words_with_offsets]\n    # Count frequency of each pre-tokenized word\n    for word in new_words:\n        word_freqs[word] += 1\n\nprint(word_freqs)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Extract the base alphabet from all words\n# The alphabet consists of all unique characters that appear in the corpus\n# This forms the initial vocabulary before any merges\nalphabet = []\n\nfor word in word_freqs.keys():\n    for letter in word:\n        if letter not in alphabet:\n            alphabet.append(letter)\nalphabet.sort()\n\nprint(alphabet)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Initialize vocabulary with special tokens and base alphabet\n# \"<|endoftext|>\" is GPT-2's special token for end of document\n# The base vocabulary starts with special tokens plus all individual characters\nvocab = [\"<|endoftext|>\"] + alphabet.copy()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Split each word into individual characters for BPE training\n# This creates the initial splits that BPE will iteratively merge\n# Each word becomes a list of characters that can be progressively combined\nsplits = {word: [c for c in word] for word in word_freqs.keys()}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Function to compute frequencies of adjacent character pairs\n# BPE works by finding the most frequent adjacent pair and merging it\n# This function counts how often each pair appears across all words (weighted by word frequency)\ndef compute_pair_freqs(splits):\n    pair_freqs = defaultdict(int)\n    for word, freq in word_freqs.items():\n        split = splits[word]\n        if len(split) == 1:\n            continue  # Skip single-character words\n        # Count each adjacent pair in the word\n        for i in range(len(split) - 1):\n            pair = (split[i], split[i + 1])\n            pair_freqs[pair] += freq  # Weight by word frequency\n    return pair_freqs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Compute initial pair frequencies to see what BPE will merge first\n# This shows the character pairs that appear most frequently in our corpus\npair_freqs = compute_pair_freqs(splits)\n\n# Display the first few pairs and their frequencies\nfor i, key in enumerate(pair_freqs.keys()):\n    print(f\"{key}: {pair_freqs[key]}\")\n    if i >= 5:\n        break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 7: Find the most frequent pair to merge first\n# BPE always merges the pair with the highest frequency\n# This greedy approach builds subwords from the most common patterns\nbest_pair = \"\"\nmax_freq = None\n\nfor pair, freq in pair_freqs.items():\n    if max_freq is None or max_freq < freq:\n        best_pair = pair\n        max_freq = freq\n\nprint(best_pair, max_freq)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 8: Record the first merge and add new token to vocabulary\n# Keep track of all merges for later tokenization\n# Add the merged token to our growing vocabulary\nmerges = {(\"Ġ\", \"t\"): \"Ġt\"}\nvocab.append(\"Ġt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 9: Function to apply a merge to all word splits\n# When we decide to merge a pair, we need to update all word splits\n# This function finds all instances of the pair and combines them\ndef merge_pair(a, b, splits):\n    for word in word_freqs:\n        split = splits[word]\n        if len(split) == 1:\n            continue  # Skip single-character words\n\n        i = 0\n        while i < len(split) - 1:\n            # If we find the pair to merge, combine them\n            if split[i] == a and split[i + 1] == b:\n                split = split[:i] + [a + b] + split[i + 2 :]\n            else:\n                i += 1  # Move to next position only if no merge occurred\n        splits[word] = split\n    return splits"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 10: Apply the first merge and see the result\n# This demonstrates how \"Ġ\" + \"t\" becomes \"Ġt\" in all affected words\nsplits = merge_pair(\"Ġ\", \"t\", splits)\nprint(splits[\"Ġtrained\"])  # Example of how the merge affected this word"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 11: Continue BPE training until we reach our target vocabulary size\n# This is the main BPE training loop that iteratively merges the most frequent pairs\nvocab_size = 50\n\nwhile len(vocab) < vocab_size:\n    # Find the most frequent pair in current splits\n    pair_freqs = compute_pair_freqs(splits)\n    best_pair = \"\"\n    max_freq = None\n    for pair, freq in pair_freqs.items():\n        if max_freq is None or max_freq < freq:\n            best_pair = pair\n            max_freq = freq\n    \n    # Apply the merge and update our vocabulary and merge rules\n    splits = merge_pair(*best_pair, splits)\n    merges[best_pair] = best_pair[0] + best_pair[1]\n    vocab.append(best_pair[0] + best_pair[1])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 12: Examine the learned merge rules\n# These rules define the order and mapping of character pair merges\n# Each merge rule maps a pair of tokens to their combined form\nprint(merges)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 13: Display the final vocabulary after BPE training\n# The vocabulary now contains the original characters plus learned subwords\n# Common subwords like \"This\", \"Ġtoken\", and \"Ġthe\" were automatically discovered\nprint(vocab)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 14: Implement tokenization function using our learned BPE rules\n# This function applies all merge rules in order to tokenize new text\ndef tokenize(text):\n    # First, pre-tokenize the text using the same method as training\n    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n    \n    # Split each word into characters\n    splits = [[l for l in word] for word in pre_tokenized_text]\n    \n    # Apply all learned merges in the order they were learned\n    for pair, merge in merges.items():\n        for idx, split in enumerate(splits):\n            i = 0\n            while i < len(split) - 1:\n                if split[i] == pair[0] and split[i + 1] == pair[1]:\n                    split = split[:i] + [merge] + split[i + 2 :]\n                else:\n                    i += 1\n            splits[idx] = split\n\n    # Flatten the list of lists into a single list of tokens\n    return sum(splits, [])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 15: Test our BPE tokenizer on new text\n# Notice how it handles words that were in training vs. new words\n# \"token\" was seen in training, but \"not\" was not, so it gets split into characters\ntokenize(\"This is not a token.\")"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Byte-Pair Encoding tokenization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}