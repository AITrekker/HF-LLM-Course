{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a tokenizer, block by block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for building custom tokenizers with the tokenizers library\n# - datasets: For loading and processing text datasets  \n# - evaluate: For model evaluation metrics\n# - transformers[sentencepiece]: Core library with SentencePiece support\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the WikiText-2 dataset for tokenizer training\n# This is a larger, realistic dataset compared to our previous small corpus\n# We create a generator function for memory-efficient training\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\", split=\"train\")\n\ndef get_training_corpus():\n    # Yield data in batches of 1000 for efficient processing\n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Save the dataset to a text file for traditional training\n# Some tokenizer trainers prefer file-based input over iterators\nwith open(\"wikitext-2.txt\", \"w\", encoding=\"utf-8\") as f:\n    for i in range(len(dataset)):\n        f.write(dataset[i][\"text\"] + \"\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build a WordPiece tokenizer from scratch using the tokenizers library\n# This demonstrates how to construct tokenizers programmatically\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\n# Initialize with WordPiece model and [UNK] token for unknown words\ntokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add BERT-style normalization (handles case and accents)\n# BertNormalizer combines several text normalization steps:\n# - Lowercasing (since we specify lowercase=True)\n# - Unicode normalization\n# - Accent stripping\ntokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Build custom normalization pipeline by combining multiple steps\n# This gives you more control over the normalization process\ntokenizer.normalizer = normalizers.Sequence(\n    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the normalization pipeline\n# See how accented characters are converted to their base forms and lowercased\nprint(tokenizer.normalizer.normalize_str(\"Héllò hôw are ü?\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up BERT-style pre-tokenization\n# BertPreTokenizer splits on whitespace and punctuation, similar to our manual implementation\ntokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Use simple whitespace pre-tokenization\n# This splits only on whitespace, keeping punctuation attached to words\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the pre-tokenization step\n# Compare how different pre-tokenizers handle the same text\ntokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate WhitespaceSplit pre-tokenizer (keeps punctuation with words)\n# Different from Whitespace - see how punctuation is handled differently\npre_tokenizer = pre_tokenizers.WhitespaceSplit()\npre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a sequence of pre-tokenizers for more sophisticated splitting\n# First split on whitespace, then separate punctuation\n# This achieves similar results to BertPreTokenizer through composition\npre_tokenizer = pre_tokenizers.Sequence(\n    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n)\npre_tokenizer.pre_tokenize_str(\"Let's test my pre-tokenizer.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure the WordPiece trainer with BERT's special tokens\n# Special tokens serve specific purposes:\n# - [UNK]: Unknown/out-of-vocabulary words\n# - [PAD]: Padding for batch processing\n# - [CLS]: Classification token (sequence start)\n# - [SEP]: Separator token (sequence end/boundary)\n# - [MASK]: Masking token for MLM training\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the tokenizer using the iterator-based approach\n# This is memory-efficient for large datasets as it doesn't load everything at once\ntokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Train from text file\n# Reset the model and train from the saved file instead\ntokenizer.model = models.WordPiece(unk_token=\"[UNK]\")\ntokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the trained tokenizer\n# Notice the WordPiece subword segmentation with \"##\" prefixes\nencoding = tokenizer.encode(\"Let's test this tokenizer.\")\nprint(encoding.tokens)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find the token IDs for special tokens needed for post-processing\n# We'll need these IDs to configure the template processor\ncls_token_id = tokenizer.token_to_id(\"[CLS]\")\nsep_token_id = tokenizer.token_to_id(\"[SEP]\")\nprint(cls_token_id, sep_token_id)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure post-processing to add special tokens like BERT\n# Template processing automatically adds [CLS] and [SEP] tokens\n# Single sequence: [CLS] text [SEP]\n# Pair of sequences: [CLS] text1 [SEP] text2 [SEP]\n# The \":0\" and \":1\" specify token type IDs for distinguishing sequences\ntokenizer.post_processor = processors.TemplateProcessing(\n    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test single sequence processing\n# Notice how [CLS] and [SEP] tokens are automatically added\nencoding = tokenizer.encode(\"Let's test this tokenizer.\")\nprint(encoding.tokens)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test pair of sequences processing\n# BERT format: [CLS] sentence1 [SEP] sentence2 [SEP]\n# type_ids help distinguish which tokens belong to which sequence\nencoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences.\")\nprint(encoding.tokens)\nprint(encoding.type_ids)  # 0 for first sequence, 1 for second sequence"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure decoder to reconstruct text from tokens\n# WordPiece decoder knows how to handle \"##\" prefixes properly\ntokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test decoding - convert token IDs back to readable text\n# The decoder properly reconstructs words from subword pieces\ntokenizer.decode(encoding.ids)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the complete tokenizer to JSON format\n# This preserves all configuration: model, normalizer, pre-tokenizer, post-processor, decoder\ntokenizer.save(\"tokenizer.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the tokenizer from saved file\n# Demonstrate that the tokenizer can be fully reconstructed\nnew_tokenizer = Tokenizer.from_file(\"tokenizer.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Wrap the fast tokenizer in Transformers' interface\n# PreTrainedTokenizerFast provides compatibility with Transformers models\n# Specify all special tokens for proper integration\nfrom transformers import PreTrainedTokenizerFast\n\nwrapped_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    # tokenizer_file=\"tokenizer.json\", # Alternative: load directly from file\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Use BERT-specific tokenizer wrapper\n# BertTokenizerFast automatically sets appropriate defaults for BERT models\nfrom transformers import BertTokenizerFast\n\nwrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now build a BPE tokenizer similar to GPT-2\n# Start fresh with the BPE model\ntokenizer = Tokenizer(models.BPE())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up BPE-style pre-tokenization with ByteLevel\n# ByteLevel ensures every possible input can be encoded (no unknown bytes)\n# add_prefix_space=False means we don't add space at the beginning\ntokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test ByteLevel pre-tokenization\n# Notice 'Ġ' represents spaces in BPE, and how it handles word boundaries\ntokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train BPE tokenizer with GPT-2 style configuration\n# Use <|endoftext|> as the special token for document boundaries\ntrainer = trainers.BpeTrainer(vocab_size=25000, special_tokens=[\"<|endoftext|>\"])\ntokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Train BPE from file\n# Reset the BPE model and train from the saved file\ntokenizer.model = models.BPE()\ntokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the trained BPE tokenizer\n# Notice the BPE subword segmentation without \"##\" prefixes (that's WordPiece-specific)\nencoding = tokenizer.encode(\"Let's test this tokenizer.\")\nprint(encoding.tokens)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure ByteLevel post-processing for BPE\n# This handles the proper mapping between byte-level and character-level representations\n# trim_offsets=False preserves original character positions\ntokenizer.post_processor = processors.ByteLevel(trim_offsets=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate offset preservation with ByteLevel processing\n# Offsets allow us to map tokens back to their original positions in the text\nsentence = \"Let's test this tokenizer.\"\nencoding = tokenizer.encode(sentence)\nstart, end = encoding.offsets[4]  # Get position of 5th token\nsentence[start:end]  # Extract original text for that token"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure ByteLevel decoder for BPE\n# This properly reconstructs text from BPE tokens, handling byte-level encoding\ntokenizer.decoder = decoders.ByteLevel()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test BPE decoding\n# The decoder converts token IDs back to the original text\ntokenizer.decode(encoding.ids)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Wrap BPE tokenizer in Transformers interface\n# Configure with GPT-2 style special tokens\nfrom transformers import PreTrainedTokenizerFast\n\nwrapped_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    bos_token=\"<|endoftext|>\",  # Beginning of sequence\n    eos_token=\"<|endoftext|>\",  # End of sequence\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Use GPT-2 specific tokenizer wrapper\n# GPT2TokenizerFast automatically sets appropriate defaults for GPT-2 models\nfrom transformers import GPT2TokenizerFast\n\nwrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Finally, build a Unigram tokenizer similar to T5/XLNet\n# Start with the Unigram model\ntokenizer = Tokenizer(models.Unigram())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up sophisticated normalization pipeline for Unigram\n# This handles various text normalization tasks for SentencePiece compatibility\nfrom tokenizers import Regex\n\ntokenizer.normalizer = normalizers.Sequence(\n    [\n        normalizers.Replace(\"``\", '\"'),        # Convert double backticks to quotes\n        normalizers.Replace(\"''\", '\"'),        # Convert double single quotes to quotes  \n        normalizers.NFKD(),                    # Unicode normalization\n        normalizers.StripAccents(),            # Remove accent marks\n        normalizers.Replace(Regex(\" {2,}\"), \" \"),  # Collapse multiple spaces\n    ]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Metaspace pre-tokenization for Unigram/SentencePiece\n# Metaspace converts spaces to '▁' characters, preserving space information\ntokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Metaspace pre-tokenization  \n# Notice the '▁' character representing spaces in SentencePiece format\ntokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test the pre-tokenizer!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Unigram tokenizer with comprehensive special tokens\n# Include tokens commonly used in various NLP tasks\nspecial_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]\ntrainer = trainers.UnigramTrainer(\n    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n)\ntokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Train Unigram from file\n# Reset the model and train from the saved file\ntokenizer.model = models.Unigram()\ntokenizer.train([\"wikitext-2.txt\"], trainer=trainer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the trained Unigram tokenizer\n# Notice the SentencePiece-style segmentation with '▁' space markers\nencoding = tokenizer.encode(\"Let's test this tokenizer.\")\nprint(encoding.tokens)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get special token IDs for Unigram post-processing\n# Find the IDs for tokens we'll use in the template\ncls_token_id = tokenizer.token_to_id(\"<cls>\")\nsep_token_id = tokenizer.token_to_id(\"<sep>\")\nprint(cls_token_id, sep_token_id)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure post-processing with custom template\n# This example puts <cls> at the end (uncommon but demonstrates flexibility)\n# Single: text <sep> <cls>\n# Pair: text1 <sep> text2 <sep> <cls>\ntokenizer.post_processor = processors.TemplateProcessing(\n    single=\"$A:0 <sep>:0 <cls>:2\",      # Type 2 for special classification token\n    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)],\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test Unigram with custom post-processing template\n# Shows how flexible template processing can create different token arrangements\nencoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\nprint(encoding.tokens)\nprint(encoding.type_ids)  # Notice the different type ID pattern: 0, 1, 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Metaspace decoder for Unigram\n# This properly handles the '▁' space markers used by SentencePiece\ntokenizer.decoder = decoders.Metaspace()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Wrap Unigram tokenizer in Transformers interface  \n# Configure with all the special tokens and padding preferences\nfrom transformers import PreTrainedTokenizerFast\n\nwrapped_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=tokenizer,\n    bos_token=\"<s>\",           # Beginning of sequence\n    eos_token=\"</s>\",          # End of sequence  \n    unk_token=\"<unk>\",         # Unknown token\n    pad_token=\"<pad>\",         # Padding token\n    cls_token=\"<cls>\",         # Classification token\n    sep_token=\"<sep>\",         # Separator token\n    mask_token=\"<mask>\",       # Mask token\n    padding_side=\"left\",       # Pad on the left (common for some models)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Use XLNet-specific tokenizer wrapper\n# XLNetTokenizerFast automatically sets appropriate defaults for XLNet/SentencePiece models\nfrom transformers import XLNetTokenizerFast\n\nwrapped_tokenizer = XLNetTokenizerFast(tokenizer_object=tokenizer)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Building a tokenizer, block by block",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}