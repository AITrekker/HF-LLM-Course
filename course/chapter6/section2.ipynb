{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a new tokenizer from an old one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for training tokenizers and working with datasets\n!uv pip install datasets evaluate transformers[sentencepiece]\n!apt install git-lfs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Git credentials for pushing to Hugging Face Hub (required for model sharing)\n!git config --global user.email \"you@example.com\"\n!git config --global user.name \"Your Name\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Hugging Face Hub to enable pushing models/tokenizers\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the CodeSearchNet dataset - contains Python code functions for training\n# This dataset will be used to create a specialized tokenizer for code\nfrom datasets import load_dataset\n\n# This can take a few minutes to load, so grab a coffee or tea while you wait!\nraw_datasets = load_dataset(\"code_search_net\", \"python\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the structure of our training dataset\n# Shows features available and the number of Python function examples\nraw_datasets[\"train\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Look at an example function from the dataset\n# The 'whole_func_string' contains complete Python functions with docstrings\nprint(raw_datasets[\"train\"][123456][\"whole_func_string\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# WARNING: Don't load entire dataset into memory at once!\n# This would create a list containing ALL function strings, consuming too much RAM\n# Don't uncomment the following line unless your dataset is small!\n# training_corpus = [raw_datasets[\"train\"][i: i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000)]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a generator to efficiently stream data in batches of 1000 functions\n# This avoids loading the entire dataset into memory at once\ntraining_corpus = (\n    raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n    for i in range(0, len(raw_datasets[\"train\"]), 1000)\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstration: Generators can only be consumed once!\n# After iterating through a generator, it's exhausted and returns empty\ngen = (i for i in range(10))\nprint(list(gen))  # First iteration: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(list(gen))  # Second iteration: [] (empty!)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Better approach: Create a function that returns a fresh generator each time\n# This allows us to iterate through the data multiple times during training\ndef get_training_corpus():\n    return (\n        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n    )\n\n\ntraining_corpus = get_training_corpus()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative implementation: More explicit generator function\n# Yields batches of function strings for tokenizer training\ndef get_training_corpus():\n    dataset = raw_datasets[\"train\"]\n    for start_idx in range(0, len(dataset), 1000):\n        samples = dataset[start_idx : start_idx + 1000]\n        yield samples[\"whole_func_string\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the base GPT-2 tokenizer that we'll adapt for Python code\n# GPT-2 was trained on general text, not code, so it's not optimal for programming languages\nfrom transformers import AutoTokenizer\n\nold_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test how the original GPT-2 tokenizer handles Python code\n# Notice how it splits \"numbers\" into multiple tokens and handles indentation poorly\nexample = '''def add_numbers(a, b):\n    \"\"\"Add the two numbers `a` and `b`.\"\"\"\n    return a + b'''\n\ntokens = old_tokenizer.tokenize(example)\ntokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train a new tokenizer specialized for Python code\n# 52000 is the new vocabulary size - larger than GPT-2's 50257 to include more code-specific tokens\ntokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the new tokenizer on the same Python code example\n# Notice improved tokenization: \"numbers\" is now a single token, better indentation handling\ntokens = tokenizer.tokenize(example)\ntokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare token efficiency: new tokenizer uses fewer tokens for the same code\n# Fewer tokens = more efficient processing and better context understanding\nprint(len(tokens))  # New tokenizer: 27 tokens\nprint(len(old_tokenizer.tokenize(example)))  # Old tokenizer: 36 tokens"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test on a more complex Python class example\n# Notice how the new tokenizer better handles Python-specific patterns like class definitions,\n# method names (__init__, __call__), and common libraries (torch)\nexample = \"\"\"class LinearLayer():\n    def __init__(self, input_size, output_size):\n        self.weight = torch.randn(input_size, output_size)\n        self.bias = torch.zeros(output_size)\n\n    def __call__(self, x):\n        return x @ self.weights + self.bias\n    \"\"\"\ntokenizer.tokenize(example)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the new tokenizer locally for future use\n# This creates a folder with all necessary tokenizer files\ntokenizer.save_pretrained(\"code-search-net-tokenizer\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login again if needed before pushing to Hub\n# Some authentication tokens may have expired during the training process\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload the tokenizer to Hugging Face Hub for sharing and reuse\n# This makes it available for others to download and use in their projects\ntokenizer.push_to_hub(\"code-search-net-tokenizer\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the tokenizer from Hugging Face Hub to verify it was uploaded correctly\n# Replace \"huggingface-course\" with your actual namespace to use your own tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Training a new tokenizer from an old one",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}