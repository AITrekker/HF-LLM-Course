{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A full training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, evaluation, and accelerate\n!uv pip install datasets evaluate transformers[sentencepiece]\n!uv pip install accelerate\n# To run the training on TPU, you will need to uncomment the following line:\n# !uv pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete setup: load and preprocess the MRPC dataset\n# This is the same preprocessing pipeline we've used before\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Define tokenization function for sentence pairs\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n\n# Apply tokenization to all dataset splits\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n# Set up data collator for dynamic padding during training\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare datasets for PyTorch training loop\n# Remove text columns and rename 'label' to 'labels' (required by PyTorch models)\n# Set format to 'torch' to return PyTorch tensors instead of lists\ntokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\ntokenized_datasets.set_format(\"torch\")\ntokenized_datasets[\"train\"].column_names"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create PyTorch DataLoaders for training and evaluation\n# shuffle=True for training to randomize examples, batch_size=8 for small batches\n# collate_fn handles dynamic padding within each batch\nfrom torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the DataLoader by examining one batch\n# This shows the structure and shapes of tensors in each batch\nfor batch in train_dataloader:\n    break\n{k: v.shape for k, v in batch.items()}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the pre-trained model for sequence classification\n# This model will be fine-tuned on our MRPC dataset\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the model with one batch to verify it works\n# This shows the loss value and output shape (8 examples, 2 classes)\noutputs = model(**batch)\nprint(outputs.loss, outputs.logits.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up the AdamW optimizer\n# lr=5e-5 is a good learning rate for fine-tuning BERT models\nfrom transformers import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up learning rate scheduler\n# Linear scheduler with warmup - gradually increases then decreases learning rate\nfrom transformers import get_scheduler\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)  # Total steps for 3 epochs\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,  # No warmup steps\n    num_training_steps=num_training_steps,\n)\nprint(num_training_steps)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Move model to GPU if available, otherwise use CPU\n# This significantly speeds up training if GPU is available\nimport torch\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\ndevice"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete training loop with progress bar\n# This is the full PyTorch training loop from scratch\nfrom tqdm.auto import tqdm\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()  # Set model to training mode\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        # Move batch to the same device as model\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()  # Compute gradients\n\n        optimizer.step()  # Update model parameters\n        lr_scheduler.step()  # Update learning rate\n        optimizer.zero_grad()  # Clear gradients for next iteration\n        progress_bar.update(1)  # Update progress bar"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the trained model on validation set\n# This computes accuracy and F1 score using the evaluate library\nimport evaluate\n\nmetric = evaluate.load(\"glue\", \"mrpc\")\nmodel.eval()  # Set model to evaluation mode\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():  # Disable gradient computation for efficiency\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)  # Convert logits to predicted classes\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()  # Calculate final metrics"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative training approach with different learning rate\n# This demonstrates how changing hyperparameters affects training\nfrom transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\noptimizer = AdamW(model.parameters(), lr=3e-5)  # Slightly lower learning rate\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Same training setup as before\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dataloader:\n        batch = {k: v.to(device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Using ðŸ¤— Accelerate for distributed training and mixed precision\n# Accelerate simplifies multi-GPU training and other optimizations\nfrom accelerate import Accelerator\nfrom transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n\naccelerator = Accelerator()  # Initialize accelerator\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\n# Prepare all objects for accelerated training (handles device placement automatically)\ntrain_dl, eval_dl, model, optimizer = accelerator.prepare(\n    train_dataloader, eval_dataloader, model, optimizer\n)\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dl)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for batch in train_dl:\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)  # Use accelerator's backward method\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Launch training function with notebook_launcher\n# This enables distributed training across multiple GPUs if available\nfrom accelerate import notebook_launcher\n\nnotebook_launcher(training_function)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "A full training",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}