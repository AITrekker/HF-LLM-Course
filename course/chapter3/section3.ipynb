{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a model with the Trainer API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, and evaluation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up the complete pipeline for fine-tuning with Trainer API\n# Load and preprocess the MRPC dataset from GLUE benchmark\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\n# Define tokenization function for sentence pairs\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n\n# Apply tokenization to all dataset splits\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n# Set up data collator for dynamic padding during training\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure basic training arguments\n# \"test-trainer\" is the output directory where model checkpoints will be saved\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\"test-trainer\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-trained model for sequence classification\n# num_labels=2 because MRPC is a binary classification task (equivalent/not equivalent)\nfrom transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the Trainer with model, arguments, and datasets\n# Trainer handles the training loop, evaluation, and logging automatically\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    processing_class=tokenizer,  # Updated parameter name for tokenizer\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start the training process\n# This will fine-tune the pre-trained BERT model on the MRPC dataset\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate predictions on the validation set\n# predictions.predictions contains raw logits, predictions.label_ids contains true labels\npredictions = trainer.predict(tokenized_datasets[\"validation\"])\nprint(predictions.predictions.shape, predictions.label_ids.shape)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert logits to predicted class labels\n# argmax finds the class with highest probability for each example\nimport numpy as np\n\npreds = np.argmax(predictions.predictions, axis=-1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate model performance using GLUE metrics\n# For MRPC task, this returns accuracy and F1 score\nimport evaluate\n\nmetric = evaluate.load(\"glue\", \"mrpc\")\nmetric.compute(predictions=preds, references=predictions.label_ids)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a function to compute metrics during training\n# This function will be called automatically during evaluation\ndef compute_metrics(eval_preds):\n    metric = evaluate.load(\"glue\", \"mrpc\")\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up training with automatic evaluation after each epoch\n# evaluation_strategy=\"epoch\" runs evaluation at the end of each training epoch\ntraining_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    processing_class=tokenizer,\n    compute_metrics=compute_metrics,  # Add metrics computation during training\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train the model with evaluation after each epoch\n# This will show accuracy and F1 scores during training\ntrainer.train()"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fine-tuning a model with the Trainer API or Keras",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}