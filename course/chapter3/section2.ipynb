{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, and evaluation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Basic PyTorch training setup for sequence classification\nimport torch\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n\n# Same as before - load pre-trained BERT model and tokenizer\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint)\nsequences = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"This course is amazing!\",\n]\nbatch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n\n# This is new - add labels for training (1 = positive sentiment for both)\nbatch[\"labels\"] = torch.tensor([1, 1])\n\n# Set up optimizer and perform one training step\noptimizer = AdamW(model.parameters())\nloss = model(**batch).loss  # Forward pass and calculate loss\nloss.backward()  # Compute gradients\noptimizer.step()  # Update model weights"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the GLUE MRPC dataset - Microsoft Research Paraphrase Corpus\n# This dataset contains sentence pairs labeled as paraphrases or not\nfrom datasets import load_dataset\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")\nraw_datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the structure of the training data\n# Each example contains two sentences and a label (0=not equivalent, 1=equivalent)\nraw_train_dataset = raw_datasets[\"train\"]\nraw_train_dataset[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect the dataset features and data types\n# ClassLabel shows the possible values: 0=not_equivalent, 1=equivalent\nraw_train_dataset.features"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize tokenizer and tokenize sentences separately (inefficient approach)\n# This demonstrates tokenizing each sentence individually\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\ntokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\ntokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Better approach: tokenize sentence pairs together\n# This creates proper input with [CLS] sentence1 [SEP] sentence2 [SEP] format\ninputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\ninputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert token IDs back to readable tokens to understand the structure\n# Notice [CLS] at start, [SEP] between and after sentences\ntokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize entire dataset with padding and truncation\n# padding=True ensures all sequences have same length, truncation=True cuts long sequences\ntokenized_dataset = tokenizer(\n    raw_datasets[\"train\"][\"sentence1\"],\n    raw_datasets[\"train\"][\"sentence2\"],\n    padding=True,\n    truncation=True,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a function to tokenize examples in batches\n# This function will be applied to the dataset using map()\ndef tokenize_function(example):\n    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply tokenization to all dataset splits (train, validation, test)\n# batched=True processes multiple examples at once for efficiency\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\ntokenized_datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up data collator for dynamic padding\n# This pads sequences to the same length within each batch (more efficient than global padding)\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare a sample batch to demonstrate dynamic padding\n# Remove text columns and keep only tokenized features needed for training\nsamples = tokenized_datasets[\"train\"][:8]\nsamples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n[len(x) for x in samples[\"input_ids\"]]  # Show different sequence lengths"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply data collator to create a padded batch\n# All sequences are now padded to the same length (67 tokens in this case)\nbatch = data_collator(samples)\n{k: v.shape for k, v in batch.items()}"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Processing the data (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}