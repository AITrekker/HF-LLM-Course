{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do when you get an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, evaluation, and git-lfs for large file storage\n!uv pip install datasets evaluate transformers[sentencepiece]\n!apt install git-lfs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Git with your credentials - required for pushing models to the Hub\n# Replace with your actual email and name\n!git config --global user.email \"you@example.com\"\n!git config --global user.name \"Your Name\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Hugging Face Hub - you'll need your access token\n# This allows you to download private models and upload your own models\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Duplicate login cell (can be removed)\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to copy a model repository template and create a new repository\n# This demonstrates how to clone an existing model and create your own copy\nfrom distutils.dir_util import copy_tree\nfrom huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name\n\n\ndef copy_repository_template():\n    # Clone the repo and extract the local path\n    # Using a specific commit hash ensures reproducibility\n    template_repo_id = \"lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\"\n    commit_hash = \"be3eaffc28669d7932492681cd5f3e8905e358b4\"\n    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)\n    \n    # Create an empty repo on the Hub with the same name\n    model_name = template_repo_id.split(\"/\")[1]\n    create_repo(model_name, exist_ok=True)\n    \n    # Clone the empty repo locally for modification\n    new_repo_id = get_full_repo_name(model_name)\n    new_repo_dir = model_name\n    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)\n    \n    # Copy all files from template to new repo\n    copy_tree(template_repo_dir, new_repo_dir)\n    \n    # Push to Hub - uploads all copied files\n    repo.push_to_hub()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Attempt to load a question-answering pipeline with the copied model\n# This will fail because the model is missing a config.json file\nfrom transformers import pipeline\n\nmodel_checkpoint = get_full_repo_name(\"distillbert-base-uncased-finetuned-squad-d5716d28\")\nreader = pipeline(\"question-answering\", model=model_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Second attempt - still fails for the same reason (missing config.json)\n# The error message helps us understand what's missing\nmodel_checkpoint = get_full_repo_name(\"distilbert-base-uncased-finetuned-squad-d5716d28\")\nreader = pipeline(\"question-answering\", model=model_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Investigate what files are actually in the repository\n# This helps us confirm that config.json is missing\nfrom huggingface_hub import list_repo_files\n\nlist_repo_files(repo_id=model_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a config from the base model that this model was fine-tuned from\n# The config.json file contains essential model architecture information\nfrom transformers import AutoConfig\n\npretrained_checkpoint = \"distilbert-base-uncased\"\nconfig = AutoConfig.from_pretrained(pretrained_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Push the config to the repository to fix the missing config.json issue\n# This adds the essential configuration file needed for model loading\nconfig.push_to_hub(model_checkpoint, commit_message=\"Add config.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now we can successfully load the question-answering pipeline\n# Using revision=\"main\" ensures we get the latest version with config.json\nreader = pipeline(\"question-answering\", model=model_checkpoint, revision=\"main\")\n\n# Test the pipeline with sample text about extractive question answering\ncontext = r\"\"\"\nExtractive Question Answering is the task of extracting an answer from a text\ngiven a question. An example of a question answering dataset is the SQuAD\ndataset, which is entirely based on that task. If you would like to fine-tune a\nmodel on a SQuAD task, you may leverage the\nexamples/pytorch/question-answering/run_squad.py script.\n\nðŸ¤— Transformers is interoperable with the PyTorch, TensorFlow, and JAX\nframeworks, so you can use your favourite tools for a wide variety of tasks!\n\"\"\"\n\nquestion = \"What is extractive question answering?\"\nreader(question=question, context=context)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract the tokenizer and model from the pipeline for manual debugging\n# This gives us direct access to the underlying components\ntokenizer = reader.tokenizer\nmodel = reader.model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare a new question to test manual processing\nquestion = \"Which frameworks can I use?\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Manual question answering processing - demonstrates what happens inside the pipeline\n# This will fail because inputs are not properly converted to tensors\nimport torch\n\n# Tokenize the question and context together\ninputs = tokenizer(question, context, add_special_tokens=True)\ninput_ids = inputs[\"input_ids\"][0]  # This line will cause an error\n\n# Forward pass through the model\noutputs = model(**inputs)\nanswer_start_scores = outputs.start_logits\nanswer_end_scores = outputs.end_logits\n\n# Find the most likely start and end positions\nanswer_start = torch.argmax(answer_start_scores)\nanswer_end = torch.argmax(answer_end_scores) + 1\n\n# Convert token IDs back to text\nanswer = tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Investigate the structure of tokenizer output\n# Check the first 5 tokens and data type to understand the issue\ninputs[\"input_ids\"][:5]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the data type of input_ids\n# The issue is that input_ids is a list, not a tensor\ntype(inputs[\"input_ids\"])"
  }
 ],
 "metadata": {
  "colab": {
   "name": "What to do when you get an error",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}