{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for Transformers, datasets, and evaluation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up a fine-tuning pipeline for MNLI (Multi-Genre Natural Language Inference)\n# This example demonstrates common training pipeline issues and their solutions\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\n\n# Load the MNLI dataset - a benchmark for natural language inference\nraw_datasets = load_dataset(\"glue\", \"mnli\")\n\n# Use DistilBERT as the base model - efficient and effective for classification\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n\ndef preprocess_function(examples):\n    \"\"\"\n    Tokenize premise and hypothesis pairs for NLI task\n    - premise: the initial statement\n    - hypothesis: the statement to be evaluated against the premise\n    - truncation=True prevents sequences from being too long\n    \"\"\"\n    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n\n\n# Apply preprocessing to all examples in the dataset\ntokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n\n# Load model for sequence classification (3 classes: entailment, neutral, contradiction)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n\n# Define training parameters\nargs = TrainingArguments(\n    f\"distilbert-finetuned-mnli\",\n    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n    save_strategy=\"epoch\",        # Save checkpoint after each epoch\n    learning_rate=2e-5,          # Learning rate for AdamW optimizer\n    num_train_epochs=3,          # Number of training epochs\n    weight_decay=0.01,           # Regularization parameter\n)\n\n# Load evaluation metric for MNLI\nmetric = evaluate.load(\"glue\", \"mnli\")\n\n\ndef compute_metrics(eval_pred):\n    \"\"\"Calculate accuracy for evaluation\"\"\"\n    predictions, labels = eval_pred\n    return metric.compute(predictions=predictions, references=labels)\n\n\n# Create trainer with raw datasets (this will cause an error)\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=raw_datasets[\"train\"],      # ERROR: Using raw instead of tokenized\n    eval_dataset=raw_datasets[\"validation_matched\"],  # ERROR: Using raw instead of tokenized\n    compute_metrics=compute_metrics,\n)\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Investigate the training dataset to understand the error\n# Let's examine what the raw dataset contains vs what the model expects\ntrainer.train_dataset[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fix #1: Use tokenized datasets instead of raw datasets\n# The model expects input_ids and attention_mask, not raw text\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\n\nraw_datasets = load_dataset(\"glue\", \"mnli\")\n\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n\n\ntokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n\nargs = TrainingArguments(\n    f\"distilbert-finetuned-mnli\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\nmetric = evaluate.load(\"glue\", \"mnli\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    return metric.compute(predictions=predictions, references=labels)\n\n\n# FIXED: Now using tokenized_datasets instead of raw_datasets\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],           # FIXED\n    eval_dataset=tokenized_datasets[\"validation_matched\"], # FIXED\n    compute_metrics=compute_metrics,\n)\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine a tokenized example to understand the data structure\n# This shows what the model actually receives as input\ntokenizer.decode(trainer.train_dataset[0][\"input_ids\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check what features are available in the tokenized dataset\n# This helps verify that tokenization worked correctly\ntrainer.train_dataset[0].keys()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the model type to understand what it expects\n# This confirms we're using the right model architecture\ntype(trainer.model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the attention mask to understand sequence lengths\n# Each example has varying length, which will cause batching issues\ntrainer.train_dataset[0][\"attention_mask\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify that input_ids and attention_mask have the same length\n# This is a crucial requirement for transformer models\nlen(trainer.train_dataset[0][\"attention_mask\"]) == len(\n    trainer.train_dataset[0][\"input_ids\"]\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the label value - should be 0, 1, or 2 for the 3 MNLI classes\ntrainer.train_dataset[0][\"label\"]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the label names to understand the classification task\n# MNLI has 3 classes: entailment, neutral, contradiction\ntrainer.train_dataset.features[\"label\"].names"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Try to get a batch from the dataloader to identify the batching issue\n# This will fail because sequences have different lengths and can't be stacked\nfor batch in trainer.get_train_dataloader():\n    break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the data collator being used\n# The default collator doesn't handle variable-length sequences properly\ndata_collator = trainer.get_train_dataloader().collate_fn\ndata_collator"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fix #2: Add proper data collator for padding sequences to the same length\n# DataCollatorWithPadding handles variable-length sequences in batches\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,  # NEW: Import the padding collator\n    TrainingArguments,\n    Trainer,\n)\n\nraw_datasets = load_dataset(\"glue\", \"mnli\")\n\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n\n\ntokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n\nargs = TrainingArguments(\n    f\"distilbert-finetuned-mnli\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\nmetric = evaluate.load(\"glue\", \"mnli\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    return metric.compute(predictions=predictions, references=labels)\n\n\n# NEW: Create a data collator that pads sequences to the same length\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# FIXED: Add data_collator and tokenizer to the trainer\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation_matched\"],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,  # NEW: Handles variable-length sequences\n    tokenizer=tokenizer,          # NEW: Provides tokenizer to trainer\n)\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the data collator by manually creating a batch\n# This verifies that the padding collator works correctly\ndata_collator = trainer.get_train_dataloader().collate_fn\nbatch = data_collator([trainer.train_dataset[i] for i in range(4)])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test with properly processed training set\n# The trainer removes unused columns automatically\ndata_collator = trainer.get_train_dataloader().collate_fn\nactual_train_set = trainer._remove_unused_columns(trainer.train_dataset)\nbatch = data_collator([actual_train_set[i] for i in range(4)])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Successfully get a batch from the training dataloader\n# This confirms that the padding collator fixed the batching issue\nfor batch in trainer.get_train_dataloader():\n    break"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test forward pass on CPU to avoid GPU memory issues\n# This will reveal the next issue: incorrect number of labels\noutputs = trainer.model.cpu()(**batch)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the model's expected number of labels\n# DistilBERT-base defaults to 2 labels, but MNLI has 3 classes\ntrainer.model.config.num_labels"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fix #3: Specify the correct number of labels (3 for MNLI)\n# The model needs to match the number of classes in the dataset\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n)\n\nraw_datasets = load_dataset(\"glue\", \"mnli\")\n\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n\n\ntokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n\n# FIXED: Specify num_labels=3 for the three MNLI classes\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n\nargs = TrainingArguments(\n    f\"distilbert-finetuned-mnli\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\nmetric = evaluate.load(\"glue\", \"mnli\")\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation_matched\"],\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test forward pass with corrected model\n# Now the model should work with the 3-class MNLI labels\nfor batch in trainer.get_train_dataloader():\n    break\n\noutputs = trainer.model.cpu()(**batch)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test GPU training with proper device management\n# Move batch to GPU if available, otherwise use CPU\nimport torch\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nbatch = {k: v.to(device) for k, v in batch.items()}\n\noutputs = trainer.model.to(device)(**batch)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test backward pass (gradient computation)\n# This verifies that the loss computation works correctly\nloss = outputs.loss\nloss.backward()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test optimizer step\n# This ensures the optimization process works\ntrainer.create_optimizer()\ntrainer.optimizer.step()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Note: Full training would work now but takes too long for demonstration\n# This would run the complete training loop\n# trainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test evaluation with an incorrect compute_metrics function\n# This will fail because we're not converting logits to predictions properly\ntrainer.evaluate()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test evaluation manually to understand the issue\n# Get predictions from the model for debugging\nfor batch in trainer.get_eval_dataloader():\n    break\n\nbatch = {k: v.to(device) for k, v in batch.items()}\n\nwith torch.no_grad():\n    outputs = trainer.model(**batch)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the current compute_metrics function to see what's wrong\n# The issue is that we need to convert logits to class predictions\npredictions = outputs.logits.cpu().numpy()\nlabels = batch[\"labels\"].cpu().numpy()\n\ncompute_metrics((predictions, labels))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the shapes to understand the data structure\n# Predictions are logits (raw scores), not class indices\npredictions.shape, labels.shape"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fix #4: Convert logits to predictions in compute_metrics\n# The metric expects class indices, not raw logits\nimport numpy as np\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Convert logits to class predictions using argmax\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\n# Test the corrected function\ncompute_metrics((predictions, labels))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Complete working solution with all fixes applied\n# This version should work correctly for MNLI fine-tuning\nimport numpy as np\nfrom datasets import load_dataset\nimport evaluate\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    DataCollatorWithPadding,\n    TrainingArguments,\n    Trainer,\n)\n\nraw_datasets = load_dataset(\"glue\", \"mnli\")\n\nmodel_checkpoint = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n\ndef preprocess_function(examples):\n    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n\n\ntokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n\n# FIXED: Correct number of labels for MNLI\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n\nargs = TrainingArguments(\n    f\"distilbert-finetuned-mnli\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\nmetric = evaluate.load(\"glue\", \"mnli\")\n\n\n# FIXED: Proper conversion from logits to predictions\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels)\n\n\n# FIXED: Proper data collator for variable-length sequences\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],        # FIXED: Use tokenized data\n    eval_dataset=tokenized_datasets[\"validation_matched\"],  # FIXED: Use tokenized data\n    compute_metrics=compute_metrics,\n    data_collator=data_collator,                      # FIXED: Handle padding\n    tokenizer=tokenizer,\n)\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate 20 training steps to show the pipeline works\n# This simulates what happens during actual training\nfor batch in trainer.get_train_dataloader():\n    break\n\nbatch = {k: v.to(device) for k, v in batch.items()}\ntrainer.create_optimizer()\n\nfor _ in range(20):\n    outputs = trainer.model(**batch)\n    loss = outputs.loss\n    loss.backward()\n    trainer.optimizer.step()\n    trainer.optimizer.zero_grad()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the final evaluation with correct metrics\n# This should now work properly with the fixed compute_metrics function\nwith torch.no_grad():\n    outputs = trainer.model(**batch)\npreds = outputs.logits\nlabels = batch[\"labels\"]\n\ncompute_metrics((preds.cpu().numpy(), labels.cpu().numpy()))"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Debugging the training pipeline",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}