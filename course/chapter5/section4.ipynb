{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big data? ðŸ¤— Datasets to the rescue!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Install the Transformers, Datasets, and Evaluate libraries to run this notebook.\n\nThis notebook demonstrates how to work with very large datasets using streaming and memory-efficient techniques."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for big data processing\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install zstandard for handling compressed datasets\n# Many large datasets use zstd compression for better compression ratios\n!uv pip install zstandard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load a very large dataset (15+ million examples)\n# This PUBMED dataset contains scientific paper abstracts\n# The dataset is compressed and will be downloaded and cached locally\nfrom datasets import load_dataset\n\n# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\ndata_files = \"https://huggingface.co/datasets/qualis2006/PUBMED_title_abstracts_2020_baseline/resolve/main/PUBMED_title_abstracts_2020_baseline.jsonl.zst\"\npubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\npubmed_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the structure of a single example\n# Each example contains metadata (PMID, language) and text (title + abstract)\npubmed_dataset[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install psutil to monitor memory usage\n# This helps us understand the memory impact of loading large datasets\n!uv pip install psutil"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Monitor current memory usage\n# Shows how much RAM the process is currently using\nimport psutil\n\n# Process.memory_info is expressed in bytes, so convert to megabytes\nprint(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the size of the cached dataset file\n# Even though the dataset is large, it's efficiently stored using Apache Arrow\nprint(f\"Number of files in dataset : {pubmed_dataset.dataset_size}\")\nsize_gb = pubmed_dataset.dataset_size / (1024**3)\nprint(f\"Dataset size (cache file) : {size_gb:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark dataset iteration performance\n# Test how quickly we can iterate through the entire dataset\nimport timeit\n\ncode_snippet = \"\"\"batch_size = 1000\n\nfor idx in range(0, len(pubmed_dataset), batch_size):\n    _ = pubmed_dataset[idx:idx + batch_size]\n\"\"\"\n\ntime = timeit.timeit(stmt=code_snippet, number=1, globals=globals())\nprint(\n    f\"Iterated over {len(pubmed_dataset)} examples (about {size_gb:.1f} GB) in \"\n    f\"{time:.1f}s, i.e. {size_gb/time:.3f} GB/s\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enable streaming mode for memory-efficient processing\n# streaming=True processes data without loading everything into memory\npubmed_dataset_streamed = load_dataset(\n    \"json\", data_files=data_files, split=\"train\", streaming=True\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Access the first example from the streamed dataset\n# Uses an iterator pattern instead of direct indexing\nnext(iter(pubmed_dataset_streamed))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply transformations to streamed datasets\n# Tokenization can be applied on-the-fly without loading full dataset\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\ntokenized_dataset = pubmed_dataset_streamed.map(lambda x: tokenizer(x[\"text\"]))\nnext(iter(tokenized_dataset))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Shuffle streamed datasets for training\n# buffer_size controls how many examples are loaded for shuffling\n# Larger buffer_size gives better randomization but uses more memory\nshuffled_dataset = pubmed_dataset_streamed.shuffle(buffer_size=10_000, seed=42)\nnext(iter(shuffled_dataset))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Take a small subset for inspection or testing\n# Useful for getting a sample of a large streamed dataset\ndataset_head = pubmed_dataset_streamed.take(5)\nlist(dataset_head)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create train/validation splits from streamed data\n# skip() and take() allow splitting without loading the full dataset\n# Skip the first 1,000 examples and include the rest in the training set\ntrain_dataset = shuffled_dataset.skip(1000)\n# Take the first 1,000 examples for the validation set\nvalidation_dataset = shuffled_dataset.take(1000)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load another large dataset for demonstration\n# This legal dataset contains court opinions and legal documents\nlaw_dataset_streamed = load_dataset(\n    \"json\",\n    data_files=\"https://the-eye.eu/public/AI/pile_preliminary_components/FreeLaw_Opinions.jsonl.zst\",\n    split=\"train\",\n    streaming=True,\n)\nnext(iter(law_dataset_streamed))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Combine multiple streamed datasets\n# interleave_datasets alternates between datasets for mixed training\nfrom itertools import islice\nfrom datasets import interleave_datasets\n\ncombined_dataset = interleave_datasets([pubmed_dataset_streamed, law_dataset_streamed])\nlist(islice(combined_dataset, 2))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the complete Pile dataset (a massive text corpus)\n# This demonstrates loading multiple files for train/validation/test splits\n# The Pile is one of the largest open-source text datasets\nbase_url = \"https://the-eye.eu/public/AI/pile/\"\ndata_files = {\n    \"train\": [base_url + \"train/\" + f\"{idx:02d}.jsonl.zst\" for idx in range(30)],\n    \"validation\": base_url + \"val.jsonl.zst\",\n    \"test\": base_url + \"test.jsonl.zst\",\n}\npile_dataset = load_dataset(\"json\", data_files=data_files, streaming=True)\nnext(iter(pile_dataset[\"train\"]))"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Big data? ðŸ¤— Datasets to the rescue!",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}