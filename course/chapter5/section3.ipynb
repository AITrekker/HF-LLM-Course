{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to slice and dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Install the Transformers, Datasets, and Evaluate libraries to run this notebook.\n\nThis notebook demonstrates advanced dataset manipulation techniques including filtering, mapping, and processing with the Datasets library."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for dataset manipulation\n# Note: Using uv pip for faster package installation\n!uv pip install datasets evaluate transformers[sentencepiece]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download and extract a dataset for drug reviews analysis\n# This dataset contains patient reviews for various medications\n!wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip\"\n!unzip drugsCom_raw.zip"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load CSV files using the datasets library\n# Key parameters:\n# - \"csv\": specifies CSV format loader\n# - data_files: dictionary mapping split names to file paths\n# - delimiter=\"\\t\": specifies tab-separated values\nfrom datasets import load_dataset\n\ndata_files = {\"train\": \"drugsComTrain_raw.tsv\", \"test\": \"drugsComTest_raw.tsv\"}\n# \\t is the tab character in Python\ndrug_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a smaller sample for faster experimentation\n# shuffle() randomizes the order, select() takes the first N examples\n# This is useful for testing code on large datasets\ndrug_sample = drug_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n# Print the first few examples of the dataset\ndrug_sample[:3]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Data validation: Check for unique patient IDs\n# This ensures each row represents a unique review\n# assert will raise an error if the condition is false\nfor split in drug_dataset.keys():\n    assert len(drug_dataset[split]) == len(drug_dataset[split].unique(\"Unnamed: 0\"))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rename column for better readability\n# \"Unnamed: 0\" is not descriptive - \"patient_id\" is clearer\ndrug_dataset = drug_dataset.rename_column(\n    original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\"\n)\ndrug_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a function to convert condition names to lowercase\n# This will help standardize the data for better analysis\ndef lowercase_condition(example):\n    return {\"condition\": example[\"condition\"].lower()}\n\n# Attempt to apply the function - this will fail due to None values\ndrug_dataset.map(lowercase_condition)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define a filter function to remove rows with missing condition data\n# This prevents errors when processing the data\ndef filter_nones(x):\n    return x[\"condition\"] is not None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstration of lambda functions\n# Lambda functions are anonymous functions for simple operations\n# Example 1: Square a number\n(lambda x: x * x)(3)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example 2: Calculate triangle area using multiple parameters\n# Lambda functions can take multiple arguments\n(lambda base, height: 0.5 * base * height)(4, 8)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply filter to remove rows with None condition values\n# This uses a lambda function inline for the filtering operation\ndrug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Now apply lowercase transformation to condition names\n# map() applies the function to every example in the dataset\ndrug_dataset = drug_dataset.map(lowercase_condition)\n# Check that lowercasing worked\ndrug_dataset[\"train\"][\"condition\"][:3]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add a new feature: review length in words\n# This creates a new column with computed values\ndef compute_review_length(example):\n    return {\"review_length\": len(example[\"review\"].split())}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply the review length computation to all examples\n# The new column will be added to the dataset automatically\ndrug_dataset = drug_dataset.map(compute_review_length)\n# Inspect the first training example\ndrug_dataset[\"train\"][0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sort dataset by review length to find shortest reviews\n# This helps identify data quality issues or outliers\ndrug_dataset[\"train\"].sort(\"review_length\")[:3]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter out very short reviews (less than 30 words)\n# Short reviews typically don't contain enough information for analysis\ndrug_dataset = drug_dataset.filter(lambda x: x[\"review_length\"] > 30)\nprint(drug_dataset.num_rows)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate HTML entity decoding\n# Drug reviews often contain HTML entities that need to be converted\nimport html\n\ntext = \"I&#039;m a transformer called BERT\"\nhtml.unescape(text)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply HTML entity decoding to all reviews\n# This cleans up the text data for better processing\ndrug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Apply HTML decoding in batched mode for better performance\n# batched=True processes multiple examples at once, improving speed\nnew_drug_dataset = drug_dataset.map(\n    lambda x: {\"review\": [html.unescape(o) for o in x[\"review\"]]}, batched=True\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up tokenization for the reviews\n# Using BERT tokenizer to convert text to tokens for model training\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"review\"], truncation=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply tokenization to the entire dataset using batched processing\n# %time measures execution time - fast tokenizers are much quicker\n%time tokenized_dataset = drug_dataset.map(tokenize_function, batched=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare with slow tokenizer using multiprocessing\n# Demonstrates the speed advantage of fast tokenizers\n# num_proc=8 uses 8 CPU cores for parallel processing\nslow_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\", use_fast=False)\n\ndef slow_tokenize_function(examples):\n    return slow_tokenizer(examples[\"review\"], truncation=True)\n\ntokenized_dataset = drug_dataset.map(slow_tokenize_function, batched=True, num_proc=8)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Handle long texts with token splitting\n# return_overflowing_tokens=True splits long texts into multiple chunks\n# max_length=128 sets the maximum sequence length\ndef tokenize_and_split(examples):\n    return tokenizer(\n        examples[\"review\"],\n        truncation=True,\n        max_length=128,\n        return_overflowing_tokens=True,\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the tokenization function on a single example\n# Shows how long texts are split into multiple sequences\nresult = tokenize_and_split(drug_dataset[\"train\"][0])\n[len(inp) for inp in result[\"input_ids\"]]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply tokenization to dataset - this will fail due to mismatched lengths\n# When texts are split, the number of tokens doesn't match original examples\ntokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Solution: Remove original columns when tokenizing\n# This prevents length mismatch errors but loses original data\ntokenized_dataset = drug_dataset.map(\n    tokenize_and_split, batched=True, remove_columns=drug_dataset[\"train\"].column_names\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the effect of tokenization on dataset size\n# Dataset grows because long texts are split into multiple examples\nlen(tokenized_dataset[\"train\"]), len(drug_dataset[\"train\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Better solution: Preserve original data while handling token overflow\n# overflow_to_sample_mapping tracks which original example each token chunk came from\ndef tokenize_and_split(examples):\n    result = tokenizer(\n        examples[\"review\"],\n        truncation=True,\n        max_length=128,\n        return_overflowing_tokens=True,\n    )\n    # Extract mapping between new and old indices\n    sample_map = result.pop(\"overflow_to_sample_mapping\")\n    for key, values in examples.items():\n        result[key] = [values[i] for i in sample_map]\n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply the improved tokenization function\n# This preserves all original data while handling long sequences\ntokenized_dataset = drug_dataset.map(tokenize_and_split, batched=True)\ntokenized_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert dataset to pandas format for analysis\n# This enables use of pandas operations on the dataset\ndrug_dataset.set_format(\"pandas\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Access data as pandas DataFrame for analysis\n# This returns the data in pandas format for statistical operations\ndrug_dataset[\"train\"][:3]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract full training set as pandas DataFrame\n# This enables full pandas functionality for data analysis\ntrain_df = drug_dataset[\"train\"][:]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze condition frequency distribution\n# value_counts() shows how often each condition appears\n# This helps understand the class distribution in the dataset\nfrequencies = (\n    train_df[\"condition\"]\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n)\nfrequencies.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert pandas DataFrame back to Dataset\n# This enables us to use Datasets library features again\nfrom datasets import Dataset\n\nfreq_dataset = Dataset.from_pandas(frequencies)\nfreq_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reset format back to standard Dataset format\n# This removes the pandas formatting applied earlier\ndrug_dataset.reset_format()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create proper train/validation/test splits\n# 80% for training, 20% for validation, plus separate test set\ndrug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n# Rename the default \"test\" split to \"validation\"\ndrug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n# Add the \"test\" set to our `DatasetDict`\ndrug_dataset_clean[\"test\"] = drug_dataset[\"test\"]\ndrug_dataset_clean"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the processed dataset to disk for future use\n# This creates a local cache that can be reloaded quickly\ndrug_dataset_clean.save_to_disk(\"drug-reviews\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reload the dataset from disk to verify saving worked\n# This demonstrates how to load previously saved datasets\nfrom datasets import load_from_disk\n\ndrug_dataset_reloaded = load_from_disk(\"drug-reviews\")\ndrug_dataset_reloaded"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export each split to JSONL format for sharing or external use\n# JSONL (JSON Lines) format is convenient for streaming and processing\nfor split, dataset in drug_dataset_clean.items():\n    dataset.to_json(f\"drug-reviews-{split}.jsonl\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inspect the exported JSONL file format\n# Each line contains one JSON object representing a single example\n!head -n 1 drug-reviews-train.jsonl"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reload dataset from JSONL files to verify export worked\n# This shows how to load datasets from the exported format\ndata_files = {\n    \"train\": \"drug-reviews-train.jsonl\",\n    \"validation\": \"drug-reviews-validation.jsonl\",\n    \"test\": \"drug-reviews-test.jsonl\",\n}\ndrug_dataset_reloaded = load_dataset(\"json\", data_files=data_files)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Time to slice and dice",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}