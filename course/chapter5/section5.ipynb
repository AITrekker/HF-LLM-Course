{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Install the Transformers, Datasets, and Evaluate libraries to run this notebook.\n\nThis notebook demonstrates how to create your own dataset by collecting data from APIs, processing it, and sharing it on the Hugging Face Hub."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages for dataset creation and Git LFS\n# Git LFS is needed for uploading large files to the Hub\n!uv pip install datasets evaluate transformers[sentencepiece]\n!apt install git-lfs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "You will need to setup git, adapt your email and name in the following cell.\n\n**Important:** Configure Git with your actual email and name before proceeding."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure Git with your credentials\n# Replace with your actual email and name\n!git config --global user.email \"you@example.com\"\n!git config --global user.name \"Your Name\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials.\n\n**Required:** You must authenticate with Hugging Face Hub to upload datasets."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Hugging Face Hub\n# This will prompt for your username and token/password\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install requests library for API calls\n# We'll use this to fetch data from the GitHub API\n!uv pip install requests"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Make an API call to GitHub's REST API\n# This gets the first page of issues from the datasets repository\nimport requests\n\nurl = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\nresponse = requests.get(url)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if the API call was successful\n# Status code 200 means the request succeeded\nresponse.status_code"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the JSON response structure\n# This shows the format of GitHub issue data\nresponse.json()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up GitHub authentication for higher rate limits\n# Replace 'xxx' with your actual GitHub personal access token\nGITHUB_TOKEN = xxx  # Copy your GitHub token here\nheaders = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to fetch all issues from a GitHub repository\n# This handles pagination and rate limiting automatically\nimport time\nimport math\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\ndef fetch_issues(\n    owner=\"huggingface\",\n    repo=\"datasets\",\n    num_issues=10_000,\n    rate_limit=5_000,\n    issues_path=Path(\".\"),\n):\n    if not issues_path.is_dir():\n        issues_path.mkdir(exist_ok=True)\n\n    batch = []\n    all_issues = []\n    per_page = 100  # Number of issues to return per page\n    num_pages = math.ceil(num_issues / per_page)\n    base_url = \"https://api.github.com/repos\"\n\n    for page in tqdm(range(num_pages)):\n        # Query with state=all to get both open and closed issues\n        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n        batch.extend(issues.json())\n\n        if len(batch) > rate_limit and len(all_issues) < num_issues:\n            all_issues.extend(batch)\n            batch = []  # Flush batch for next time period\n            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n            time.sleep(60 * 60 + 1)\n\n    all_issues.extend(batch)\n    df = pd.DataFrame.from_records(all_issues)\n    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n    print(\n        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Execute the data collection function\n# This may take several minutes depending on your internet connection\n# Depending on your internet connection, this can take several minutes to run...\nfetch_issues()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the collected issues data into a Dataset\n# This converts our JSONL file into a Hugging Face Dataset\nissues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\nissues_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore the dataset to understand pull requests vs issues\n# GitHub API returns both issues and pull requests in the same endpoint\nsample = issues_dataset.shuffle(seed=666).select(range(3))\n\n# Print out the URL and pull request entries\nfor url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n    print(f\">> URL: {url}\")\n    print(f\">> Pull request: {pr}\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add a feature to distinguish issues from pull requests\n# This creates a boolean column for easier filtering\nissues_dataset = issues_dataset.map(\n    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fetch comments for a specific issue\n# This demonstrates getting additional data via API calls\nissue_number = 2792\nurl = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\nresponse = requests.get(url, headers=headers)\nresponse.json()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a function to extract comment text from issues\n# This function handles the API call and extracts only the comment body\ndef get_comments(issue_number):\n    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n    response = requests.get(url, headers=headers)\n    return [r[\"body\"] for r in response.json()]\n\n# Test our function works as expected\nget_comments(2792)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Add comments to each issue in the dataset\n# This enriches the dataset with comment data for better analysis\n# Depending on your internet connection, this can take a few minutes...\nissues_with_comments_dataset = issues_dataset.map(\n    lambda x: {\"comments\": get_comments(x[\"number\"])}\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Re-authenticate with Hugging Face Hub (if needed)\n# Sometimes authentication expires during long processing\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload the dataset to Hugging Face Hub\n# This makes your dataset publicly available and shareable\nissues_with_comments_dataset.push_to_hub(\"github-issues\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the dataset back from the Hub to verify upload\n# This demonstrates how others can access your published dataset\nremote_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\nremote_dataset"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Creating your own dataset",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}