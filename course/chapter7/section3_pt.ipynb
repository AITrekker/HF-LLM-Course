{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a masked language model (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries for masked language model fine-tuning\n!uv pip install datasets evaluate transformers[sentencepiece]\n!uv pip install accelerate\n# To run the training on TPU, you will need to uncomment the following line:\n# !uv pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n!apt install git-lfs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure git credentials for model pushing to Hugging Face Hub\n!git config --global user.email \"you@example.com\"\n!git config --global user.name \"Your Name\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login to Hugging Face Hub for model upload and access\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load DistilBERT model for masked language modeling (MLM)\n# DistilBERT is a smaller, faster version of BERT with 97% of performance\nfrom transformers import AutoModelForMaskedLM\n\nmodel_checkpoint = \"distilbert-base-uncased\"\nmodel = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare model sizes: DistilBERT vs BERT\n# DistilBERT has significantly fewer parameters while maintaining most performance\ndistilbert_num_parameters = model.num_parameters() / 1_000_000\nprint(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\nprint(f\"'>>> BERT number of parameters: 110M'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example text with [MASK] token for testing mask prediction\ntext = \"This is a great [MASK].\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the corresponding tokenizer for DistilBERT\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the pretrained model's mask filling capabilities\nimport torch\n\ninputs = tokenizer(text, return_tensors=\"pt\")\ntoken_logits = model(**inputs).logits\n# Find the location of [MASK] and extract its logits\nmask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\nmask_token_logits = token_logits[0, mask_token_index, :]\n# Pick the [MASK] candidates with the highest logits\ntop_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n\nfor token in top_5_tokens:\n    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the IMDB dataset for domain adaptation\n# We'll use movie reviews to adapt the model to the movie domain\nfrom datasets import load_dataset\n\nimdb_dataset = load_dataset(\"imdb\")\nimdb_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine sample reviews from the dataset\nsample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))\n\nfor row in sample:\n    print(f\"\\n'>>> Review: {row['text']}'\")\n    print(f\"'>>> Label: {row['label']}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenization function for the dataset\n# Includes word_ids for tracking word boundaries (used for whole word masking)\ndef tokenize_function(examples):\n    result = tokenizer(examples[\"text\"])\n    if tokenizer.is_fast:\n        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n    return result\n\n\n# Use batched=True to activate fast multithreading!\ntokenized_datasets = imdb_dataset.map(\n    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n)\ntokenized_datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check the maximum token length the model can handle\ntokenizer.model_max_length"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set chunk size for grouping text sequences\n# Smaller chunks allow for more efficient training and better GPU memory usage\nchunk_size = 128"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine the length of individual tokenized samples\n# Reviews vary significantly in length\ntokenized_samples = tokenized_datasets[\"train\"][:3]\n\nfor idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n    print(f\"'>>> Review {idx} length: {len(sample)}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Concatenate all samples into one long sequence\n# This prepares for chunking into fixed-size pieces\nconcatenated_examples = {\n    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n}\ntotal_length = len(concatenated_examples[\"input_ids\"])\nprint(f\"'>>> Concatenated reviews length: {total_length}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split the concatenated sequence into fixed-size chunks\n# Last chunk may be shorter than chunk_size\nchunks = {\n    k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n    for k, t in concatenated_examples.items()\n}\n\nfor chunk in chunks[\"input_ids\"]:\n    print(f\"'>>> Chunk length: {len(chunk)}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Function to group texts into fixed-size chunks for MLM training\ndef group_texts(examples):\n    # Concatenate all texts\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    # Compute length of concatenated texts\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the last chunk if it's smaller than chunk_size\n    total_length = (total_length // chunk_size) * chunk_size\n    # Split by chunks of max_len\n    result = {\n        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n        for k, t in concatenated_examples.items()\n    }\n    # Create a new labels column (copy of input_ids for MLM)\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply the grouping function to create datasets suitable for MLM\nlm_datasets = tokenized_datasets.map(group_texts, batched=True)\nlm_datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Examine a processed example to see how texts are concatenated and chunked\ntokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data collator for masked language modeling\n# mlm_probability=0.15 means 15% of tokens will be masked during training\nfrom transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the data collator to see how masking works\n# Random tokens are replaced with [MASK] for the model to predict\nsamples = [lm_datasets[\"train\"][i] for i in range(2)]\nfor sample in samples:\n    _ = sample.pop(\"word_ids\")\n\nfor chunk in data_collator(samples)[\"input_ids\"]:\n    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Whole Word Masking (WWM) data collator\n# This masks entire words instead of individual tokens, which is more realistic\nimport collections\nimport numpy as np\n\nfrom transformers import default_data_collator\n\nwwm_probability = 0.2\n\n\ndef whole_word_masking_data_collator(features):\n    for feature in features:\n        word_ids = feature.pop(\"word_ids\")\n\n        # Create a map between words and corresponding token indices\n        mapping = collections.defaultdict(list)\n        current_word_index = -1\n        current_word = None\n        for idx, word_id in enumerate(word_ids):\n            if word_id is not None:\n                if word_id != current_word:\n                    current_word = word_id\n                    current_word_index += 1\n                mapping[current_word_index].append(idx)\n\n        # Randomly mask words\n        mask = np.random.binomial(1, wwm_probability, (len(mapping),))\n        input_ids = feature[\"input_ids\"]\n        labels = feature[\"labels\"]\n        new_labels = [-100] * len(labels)  # -100 = ignore token\n        for word_id in np.where(mask)[0]:\n            word_id = word_id.item()\n            for idx in mapping[word_id]:\n                new_labels[idx] = labels[idx]\n                input_ids[idx] = tokenizer.mask_token_id\n        feature[\"labels\"] = new_labels\n\n    return default_data_collator(features)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the whole word masking collator\n# Notice how entire words are masked instead of individual subword tokens\nsamples = [lm_datasets[\"train\"][i] for i in range(2)]\nbatch = whole_word_masking_data_collator(samples)\n\nfor chunk in batch[\"input_ids\"]:\n    print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a smaller dataset for faster training and demonstration\ntrain_size = 10_000\ntest_size = int(0.1 * train_size)\n\ndownsampled_dataset = lm_datasets[\"train\"].train_test_split(\n    train_size=train_size, test_size=test_size, seed=42\n)\ndownsampled_dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Login again for model uploading\nfrom huggingface_hub import notebook_login\n\nnotebook_login()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure training arguments for masked language model fine-tuning\nfrom transformers import TrainingArguments\n\nbatch_size = 64\n# Show the training loss with every epoch\nlogging_steps = len(downsampled_dataset[\"train\"]) // batch_size\nmodel_name = model_checkpoint.split(\"/\")[-1]\n\ntraining_args = TrainingArguments(\n    output_dir=f\"{model_name}-finetuned-imdb\",\n    overwrite_output_dir=True,\n    evaluation_strategy=\"epoch\",  # Evaluate after each epoch\n    learning_rate=2e-5,  # Learning rate for Adam optimizer\n    weight_decay=0.01,  # L2 regularization\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    push_to_hub=True,  # Upload model to Hugging Face Hub\n    fp16=True,  # Use mixed precision for faster training\n    logging_steps=logging_steps,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the Trainer for masked language model fine-tuning\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=downsampled_dataset[\"train\"],\n    eval_dataset=downsampled_dataset[\"test\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the model before training to establish baseline perplexity\n# Perplexity measures how well the model predicts the text (lower is better)\nimport math\n\neval_results = trainer.evaluate()\nprint(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Start the fine-tuning process\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the model after training to see improvement\n# Perplexity should be lower, indicating better domain adaptation\neval_results = trainer.evaluate()\nprint(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload the fine-tuned model to Hugging Face Hub\ntrainer.push_to_hub()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Helper function to add masking to evaluation dataset\ndef insert_random_mask(batch):\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    masked_inputs = data_collator(features)\n    # Create a new \"masked\" column for each column in the dataset\n    return {\"masked_\" + k: v.numpy() for k, v in masked_inputs.items()}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare evaluation dataset with pre-applied masking for manual training loop\ndownsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\neval_dataset = downsampled_dataset[\"test\"].map(\n    insert_random_mask,\n    batched=True,\n    remove_columns=downsampled_dataset[\"test\"].column_names,\n)\neval_dataset = eval_dataset.rename_columns(\n    {\n        \"masked_input_ids\": \"input_ids\",\n        \"masked_attention_mask\": \"attention_mask\",\n        \"masked_labels\": \"labels\",\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Alternative: Manual training loop with PyTorch DataLoaders\nfrom torch.utils.data import DataLoader\nfrom transformers import default_data_collator\n\nbatch_size = 64\ntrain_dataloader = DataLoader(\n    downsampled_dataset[\"train\"],\n    shuffle=True,  # Shuffle training data\n    batch_size=batch_size,\n    collate_fn=data_collator,  # Apply masking during training\n)\neval_dataloader = DataLoader(\n    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up optimizer for manual training\nfrom torch.optim import AdamW\n\noptimizer = AdamW(model.parameters(), lr=5e-5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use Accelerate for device placement and distributed training\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up learning rate scheduler\nfrom transformers import get_scheduler\n\nnum_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",  # Linear decay\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up repository for model versioning\nfrom huggingface_hub import get_full_repo_name\n\nmodel_name = \"distilbert-base-uncased-finetuned-imdb-accelerate\"\nrepo_name = get_full_repo_name(model_name)\nrepo_name"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository for local model saving\nfrom huggingface_hub import Repository\n\noutput_dir = model_name\nrepo = Repository(output_dir, clone_from=repo_name)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Manual training loop with perplexity tracking\nfrom tqdm.auto import tqdm\nimport torch\nimport math\n\nprogress_bar = tqdm(range(num_training_steps))\n\nfor epoch in range(num_train_epochs):\n    # Training phase\n    model.train()\n    for batch in train_dataloader:\n        outputs = model(**batch)\n        loss = outputs.loss\n        accelerator.backward(loss)\n\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n\n    # Evaluation phase\n    model.eval()\n    losses = []\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n\n        loss = outputs.loss\n        losses.append(accelerator.gather(loss.repeat(batch_size)))\n\n    # Calculate perplexity (exp of average loss)\n    losses = torch.cat(losses)\n    losses = losses[: len(eval_dataset)]\n    try:\n        perplexity = math.exp(torch.mean(losses))\n    except OverflowError:\n        perplexity = float(\"inf\")\n\n    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n\n    # Save and upload model\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(output_dir)\n        repo.push_to_hub(\n            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n        )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the fine-tuned model using a pipeline\n# The model should now be better at completing movie-related sentences\nfrom transformers import pipeline\n\nmask_filler = pipeline(\n    \"fill-mask\", model=\"huggingface-course/distilbert-base-uncased-finetuned-imdb\"\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compare predictions: fine-tuned model should suggest movie-related words\npreds = mask_filler(text)\n\nfor pred in preds:\n    print(f\">>> {pred['sequence']}\")"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Fine-tuning a masked language model (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}