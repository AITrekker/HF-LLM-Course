{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required libraries using uv for faster package management\n# uv pip provides significantly faster installation compared to standard pip\n!uv pip install datasets evaluate transformers[sentencepiece]\n!uv pip install accelerate\n# To run the training on TPU, you will need to uncomment the following line:\n# !uv pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n!apt install git-lfs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the KDE4 dataset for English-French translation\n# This dataset contains parallel sentences from KDE4 localization files\n# Parameters:\n# - lang1=\"en\": Source language (English)\n# - lang2=\"fr\": Target language (French)\nfrom datasets import load_dataset\n\nraw_datasets = load_dataset(\"kde4\", lang1=\"en\", lang2=\"fr\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'translation'],\n",
       "        num_rows: 210173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split the training data into train/validation sets\n# Using 90% for training and 10% for validation\n# The seed ensures reproducible splits across runs\nsplit_datasets = raw_datasets[\"train\"].train_test_split(train_size=0.9, seed=20)\nsplit_datasets"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Rename the \"test\" split to \"validation\" for consistency\n# This follows the standard naming convention where \"validation\" is used during training\nsplit_datasets[\"validation\"] = split_datasets.pop(\"test\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Default to expanded threads',\n",
       " 'fr': 'Par défaut, développer les fils de discussion'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][1][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the pre-trained translation model on our data\n# Helsinki-NLP/opus-mt-en-fr is a pre-trained English-to-French translation model\n# This gives us a baseline to compare against our fine-tuned model\nfrom transformers import pipeline\n\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\ntranslator = pipeline(\"translation\", model=model_checkpoint)\ntranslator(\"Default to expanded threads\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'Unable to import %1 using the OFX importer plugin. This file is not the correct format.',\n",
       " 'fr': \"Impossible d'importer %1 en utilisant le module d'extension d'importation OFX. Ce fichier n'a pas un format correct.\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets[\"train\"][172][\"translation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"Impossible d'importer %1 en utilisant le plugin d'importateur OFX. Ce fichier n'est pas le bon format.\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(\n",
    "    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the tokenizer for the pre-trained model\n# The tokenizer handles text preprocessing for the model\n# return_tensors=\"pt\" ensures PyTorch tensors are returned\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, return_tensors=\"pt\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tokenize a translation pair to understand the format\n# For seq2seq models, we need both input_ids (source) and labels (target)\n# text_target parameter specifies the target text for translation tasks\nen_sentence = split_datasets[\"train\"][1][\"translation\"][\"en\"]\nfr_sentence = split_datasets[\"train\"][1][\"translation\"][\"fr\"]\n\ninputs = tokenizer(en_sentence, text_target=fr_sentence)\ninputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate the difference between correct and incorrect target tokenization\n# When using text_target, the tokenizer properly handles the target sequence\n# Without text_target, targets would include unwanted special tokens\nwrong_targets = tokenizer(fr_sentence)\nprint(tokenizer.convert_ids_to_tokens(wrong_targets[\"input_ids\"]))\nprint(tokenizer.convert_ids_to_tokens(inputs[\"labels\"]))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define preprocessing function for the entire dataset\n# This function tokenizes both source and target sentences\n# Parameters:\n# - max_length=128: Maximum sequence length to avoid memory issues\n# - truncation=True: Truncate sequences longer than max_length\nmax_length = 128\n\n\ndef preprocess_function(examples):\n    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n    targets = [ex[\"fr\"] for ex in examples[\"translation\"]]\n    model_inputs = tokenizer(\n        inputs, text_target=targets, max_length=max_length, truncation=True\n    )\n    return model_inputs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Apply preprocessing to the entire dataset\n# batched=True processes multiple examples at once for efficiency\n# remove_columns removes the original text columns, keeping only tokenized data\ntokenized_datasets = split_datasets.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=split_datasets[\"train\"].column_names,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the pre-trained model for sequence-to-sequence tasks\n# AutoModelForSeq2SeqLM automatically selects the appropriate architecture\n# This model will be fine-tuned on our translation dataset\nfrom transformers import AutoModelForSeq2SeqLM\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create data collator for sequence-to-sequence training\n# This handles padding and creates decoder_input_ids from labels\n# It ensures all sequences in a batch have the same length\nfrom transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention_mask', 'input_ids', 'labels', 'decoder_input_ids'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(1, 3)])\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100],\n",
       "        [ 1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,   817,\n",
       "           550,  7032,  5821,  7907, 12649,     0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[59513,   577,  5891,     2,  3184,    16,  2542,     5,  1710,     0,\n",
       "         59513, 59513, 59513, 59513, 59513, 59513],\n",
       "        [59513,  1211,     3,    49,  9409,  1211,     3, 29140,   817,  3124,\n",
       "           817,   550,  7032,  5821,  7907, 12649]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"decoder_input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[577, 5891, 2, 3184, 16, 2542, 5, 1710, 0]\n",
       "[1211, 3, 49, 9409, 1211, 3, 29140, 817, 3124, 817, 550, 7032, 5821, 7907, 12649, 0]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, 3):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install sacrebleu for translation evaluation\n# SACREBLEU is a standard metric for evaluating machine translation quality\n!uv pip install sacrebleu"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the SACREBLEU metric for translation evaluation\n# SACREBLEU measures translation quality by comparing n-gram overlap\n# Higher scores indicate better translation quality\nimport evaluate\n\nmetric = evaluate.load(\"sacrebleu\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 46.750469682990165,\n",
       " 'counts': [11, 6, 4, 3],\n",
       " 'totals': [12, 11, 10, 9],\n",
       " 'precisions': [91.67, 54.54, 40.0, 33.33],\n",
       " 'bp': 0.9200444146293233,\n",
       " 'sys_len': 12,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\n",
    "    \"This plugin lets you translate web pages between several languages automatically.\"\n",
    "]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 1.683602693167689,\n",
       " 'counts': [1, 0, 0, 0],\n",
       " 'totals': [4, 3, 2, 1],\n",
       " 'precisions': [25.0, 16.67, 12.5, 12.5],\n",
       " 'bp': 0.10539922456186433,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This This This This\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [2, 1, 0, 0],\n",
       " 'totals': [2, 1, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 0.004086771438464067,\n",
       " 'sys_len': 2,\n",
       " 'ref_len': 13}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"This plugin\"]\n",
    "references = [\n",
    "    [\n",
    "        \"This plugin allows you to automatically translate web pages between several languages.\"\n",
    "    ]\n",
    "]\n",
    "metric.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define function to compute BLEU scores during training\n# This function processes model predictions and computes translation quality metrics\n# It handles the conversion from token IDs back to text for evaluation\nimport numpy as np\n\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n    # In case the model returns more than the prediction logits\n    if isinstance(preds, tuple):\n        preds = preds[0]\n\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n\n    # Replace -100s in the labels as we can't decode them\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Some simple post-processing\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure training arguments for sequence-to-sequence fine-tuning\n# Key parameters:\n# - predict_with_generate=True: Use generation for evaluation instead of teacher forcing\n# - fp16=True: Enable mixed precision for faster training and lower memory usage\n# - push_to_hub=True: Automatically upload the model to Hugging Face Hub\nfrom transformers import Seq2SeqTrainingArguments\n\nargs = Seq2SeqTrainingArguments(\n    f\"marian-finetuned-kde4-en-to-fr\",\n    evaluation_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=64,\n    weight_decay=0.01,\n    save_total_limit=3,\n    num_train_epochs=3,\n    predict_with_generate=True,\n    fp16=True,\n    push_to_hub=True,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the Seq2SeqTrainer for translation model training\n# This trainer handles the complexities of sequence-to-sequence training\n# including proper loss computation and evaluation with generation\nfrom transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    model,\n    args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the model before training to establish baseline performance\n# max_length parameter ensures generated sequences don't exceed this limit\n# This gives us the pre-trained model's performance on our specific dataset\ntrainer.evaluate(max_length=max_length)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Fine-tune the translation model on our dataset\n# This will adapt the pre-trained model to our specific domain (KDE4 translations)\n# The training will run for 3 epochs as specified in the training arguments\ntrainer.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate the fine-tuned model to measure improvement\n# Compare this BLEU score with the baseline to see training effectiveness\n# Higher BLEU scores indicate better translation quality\ntrainer.evaluate(max_length=max_length)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload the fine-tuned model to Hugging Face Hub\n# This makes the model publicly available for others to use\n# Tags help with discoverability and organization\ntrainer.push_to_hub(tags=\"translation\", commit_message=\"Training complete\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data loaders for manual training loop with Accelerate\n# This demonstrates how to implement training without the Trainer class\n# Setting format to \"torch\" ensures PyTorch tensors are returned\nfrom torch.utils.data import DataLoader\n\ntokenized_datasets.set_format(\"torch\")\ntrain_dataloader = DataLoader(\n    tokenized_datasets[\"train\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=8,\n)\neval_dataloader = DataLoader(\n    tokenized_datasets[\"validation\"], collate_fn=data_collator, batch_size=8\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load a fresh model for the manual training demonstration\n# Starting from the original checkpoint to show the complete training process\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the AdamW optimizer for training\n# AdamW is recommended for transformer models due to its weight decay regularization\n# Learning rate of 2e-5 is a common choice for fine-tuning pre-trained models\nfrom transformers import AdamW\n\noptimizer = AdamW(model.parameters(), lr=2e-5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare model and data loaders with Accelerate\n# Accelerate handles device placement and distributed training automatically\n# This ensures the training code works on CPU, GPU, or multiple GPUs\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\nmodel, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set up learning rate scheduler for training\n# Linear decay from initial learning rate to 0 over the training period\n# num_warmup_steps=0 means no warmup phase\nfrom transformers import get_scheduler\n\nnum_train_epochs = 3\nnum_update_steps_per_epoch = len(train_dataloader)\nnum_training_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=num_training_steps,\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sgugger/marian-finetuned-kde4-en-to-fr-accelerate'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"marian-finetuned-kde4-en-to-fr-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "epoch 0, BLEU score: 53.47\n",
       "epoch 1, BLEU score: 54.24\n",
       "epoch 2, BLEU score: 54.44"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                max_length=128,\n",
    "            )\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(generated_tokens)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(f\"epoch {epoch}, BLEU score: {results['score']:.2f}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test the final fine-tuned model with example translations\n# This demonstrates how to use the trained model for inference\n# Replace with your own checkpoint path when using your trained model\nfrom transformers import pipeline\n\n# Replace this with your own checkpoint\nmodel_checkpoint = \"huggingface-course/marian-finetuned-kde4-en-to-fr\"\ntranslator = pipeline(\"translation\", model=model_checkpoint)\ntranslator(\"Default to expanded threads\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test translation on a more complex sentence\n# This shows how the fine-tuned model handles longer, more technical text\n# Compare the output quality with the original model's translation\ntranslator(\n    \"Unable to import %1 using the OFX importer plugin. This file is not the correct format.\"\n)"
  }
 ],
 "metadata": {
  "colab": {
   "name": "Translation (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (HF-notebooks)",
   "language": "python",
   "name": "HF-notebooks"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}